{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitacora #2 - Entrenamiento Intensivo\n",
    "**Fecha:** 2026-02-15  \n",
    "**Autor:** Francis Bravo  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cambios realizados en esta sesion\n",
    "\n",
    "### 1.1 Creacion de READMEs (29 archivos)\n",
    "Se creo un `README.md` en cada carpeta y subcarpeta del proyecto para documentar su contenido.\n",
    "\n",
    "**Carpetas principales (11):**\n",
    "- `bot/` - Implementaciones de jugadores (CNN_bot, CNN_F_bot, random_bot, human)\n",
    "- `models/` - Arquitecturas CNN (CNN1, CNN_uncoupled, CNNfrancis, CNN_fdec)\n",
    "- `QuartoRL/` - Funciones de RL (gen_experience, DQN_training_step, contest, plotting, observers)\n",
    "- `utils/` - Utilidades (env_bootstrap, logger, play_games_compat, bradley_terry)\n",
    "- `CHECKPOINTS/` - Pesos de modelos organizados por experimento\n",
    "- `analysis/` - Notebooks de analisis de experimentos\n",
    "- `development/` - Herramientas de desarrollo y debugging\n",
    "- `logs/` - Logs de entrenamiento\n",
    "- `tests/` - Notebooks de testing\n",
    "- `train_scripts/` - Scripts generados para barridos de hiperparametros\n",
    "- `partidas_guardadas/` - CSVs de partidas jugadas\n",
    "\n",
    "**Subcarpetas de analysis (10):**\n",
    "BT1, E02_win_rate, E02_wr, E03_BS, E04_N_LAST_STATES, E05_HYPERPARAMETER_ANALYSIS, E06_architecture_prebug, E08_loss-balance, legacy, XYZ\n",
    "\n",
    "**Subcarpetas de development (3):**\n",
    "01 remove files, 02_includingBT, 03_validate_deserialize_boards\n",
    "\n",
    "**Subcarpetas de CHECKPOINTS (6):**\n",
    "05_LOSS, EXP_id03, Francis, LOSS_APPROACHs_1212-2_only_select, others, REF\n",
    "\n",
    "### 1.2 Modificacion del readme.md raiz\n",
    "Se agrego:\n",
    "- Aviso destacado al inicio: solo modificar parametros de entrenamiento\n",
    "- Seccion \"Reglas de Modificacion\" con lista de parametros PERMITIDOS y carpetas PROHIBIDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parametros del entrenamiento actual\n",
    "\n",
    "Parametros configurados en ambos scripts (valores identicos).\n",
    "Cada parametro tiene hipervinculos a la linea exacta en ambos archivos:\n",
    "- [`trainRL.py`](trainRL.py) — entrenamiento desde cero\n",
    "- [`trainRL_resume_latest.py`](trainRL_resume_latest.py) — reanudar desde ultimo checkpoint\n",
    "\n",
    "### 2.1 Configuracion principal\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| STARTING_NET | `None` | [L43](trainRL.py#L43) | [L46](trainRL_resume_latest.py#L46) | Pesos aleatorios (sin checkpoint previo) |\n",
    "| EXPERIMENT_NAME | `\"05_LOSS\"` | [L44](trainRL.py#L44) | [L47](trainRL_resume_latest.py#L47) | Nombre del experimento |\n",
    "| ARCHITECTURE | `QuartoCNN_uncoupled` | [L50](trainRL.py#L50) | [L54](trainRL_resume_latest.py#L54) | Red con cabezas independientes |\n",
    "| LOSS_APPROACH | `\"combined_avg\"` | [L51](trainRL.py#L51) | [L55](trainRL_resume_latest.py#L55) | Promedio de Q-values de place y select |\n",
    "| REWARD_FUNCTION | `\"propagate\"` | [L52](trainRL.py#L52) | [L56](trainRL_resume_latest.py#L56) | Recompensa final propagada a todos los estados |\n",
    "\n",
    "### 2.2 Hiperparametros de entrenamiento\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| EPOCHS | `3000` | [L66](trainRL.py#L66) | [L70](trainRL_resume_latest.py#L70) | Epocas totales |\n",
    "| BATCH_SIZE | `30` | [L61](trainRL.py#L61) | [L65](trainRL_resume_latest.py#L65) | Muestras por batch |\n",
    "| LR | `5e-5` | [L111](trainRL.py#L111) | [L117](trainRL_resume_latest.py#L117) | Learning rate inicial |\n",
    "| LR_F | `5e-5` | [L112](trainRL.py#L112) | [L118](trainRL_resume_latest.py#L118) | Learning rate final (constante, sin decay) |\n",
    "| TAU | `0.01` | [L113](trainRL.py#L113) | [L119](trainRL_resume_latest.py#L119) | Tasa de soft update del target network |\n",
    "| GAMMA | `0.99` | [L115](trainRL.py#L115) | [L121](trainRL_resume_latest.py#L121) | Factor de descuento |\n",
    "| MAX_GRAD_NORM | `1.0` | [L110](trainRL.py#L110) | [L116](trainRL_resume_latest.py#L116) | Clipping de gradientes |\n",
    "| torch.manual_seed | `5` | [L171](trainRL.py#L171) | [L204](trainRL_resume_latest.py#L204) | Semilla para reproducibilidad |\n",
    "\n",
    "### 2.3 Generacion de experiencia\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| GEN_EXPERIENCE_BY_EPOCH | `True` | [L56](trainRL.py#L56) | [L60](trainRL_resume_latest.py#L60) | Generar experiencia nueva cada epoca |\n",
    "| MATCHES_PER_EPOCH | `100` | [L73](trainRL.py#L73) | [L77](trainRL_resume_latest.py#L77) | Partidas de self-play por epoca |\n",
    "| N_LAST_STATES_INIT | `2` | [L69](trainRL.py#L69) | [L73](trainRL_resume_latest.py#L73) | Estados del historial (inicio) |\n",
    "| N_LAST_STATES_FINAL | `2` | [L71](trainRL.py#L71) | [L75](trainRL_resume_latest.py#L75) | Estados del historial (final, sin curriculum) |\n",
    "| TEMPERATURE_EXPLORE | `2` | [L91](trainRL.py#L91) | [L95](trainRL_resume_latest.py#L95) | Temperatura para exploracion (self-play) |\n",
    "| TEMPERATURE_EXPLOIT | `0.1` | [L94](trainRL.py#L94) | [L98](trainRL_resume_latest.py#L98) | Temperatura para explotacion (evaluacion) |\n",
    "| mode_2x2 | `True` | [L63](trainRL.py#L63) | [L67](trainRL_resume_latest.py#L67) | Victoria por cuadrado 2x2 habilitada |\n",
    "\n",
    "### 2.4 Evaluacion y guardado\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| N_MATCHES_EVAL | `30` | [L59](trainRL.py#L59) | [L63](trainRL_resume_latest.py#L63) | Partidas de evaluacion vs baselines por epoca |\n",
    "| FREQ_EPOCH_SAVING | `1000` | [L96](trainRL.py#L96) | [L100](trainRL_resume_latest.py#L100) | Guardar modelo cada N epocas |\n",
    "| FREQ_EPOCH_PLOT_SHOW | `50` | [L101](trainRL.py#L101) | [L105](trainRL_resume_latest.py#L105) | Mostrar plots cada N epocas |\n",
    "| SMOOTHING_WINDOW | `10` | [L104](trainRL.py#L104) | [L110](trainRL_resume_latest.py#L110) | Ventana de suavizado para plots |\n",
    "| Q_PLOT_TYPE | `\"hist\"` | [L107](trainRL.py#L107) | [L113](trainRL_resume_latest.py#L113) | Tipo de plot de Q-values |\n",
    "\n",
    "### 2.5 Parametros exclusivos de trainRL_resume_latest.py\n",
    "\n",
    "| Parametro | Valor | Linea | Descripcion |\n",
    "|-----------|-------|-------|-------------|\n",
    "| AUTO_RESUME_LATEST | `True` | [L44](trainRL_resume_latest.py#L44) | Auto-detectar ultimo checkpoint |\n",
    "| PLOT_ON_FIRST_EPOCH | `True` | [L107](trainRL_resume_latest.py#L107) | Mostrar plots en primera epoca resumida |\n",
    "\n",
    "### 2.6 Baselines de evaluacion\n",
    "\n",
    "| Baseline | Checkpoint | trainRL | resume |\n",
    "|----------|-----------|---------|--------|\n",
    "| bot_loss-BT | `LOSS_APPROACHs_1212-2_only_select` epoch 1034 | [L119](trainRL.py#L119) | [L148](trainRL_resume_latest.py#L148) |\n",
    "| bot_random | `EXP_id03` epoch 0000 | [L130](trainRL.py#L130) | [L159](trainRL_resume_latest.py#L159) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Valores derivados\n",
    "\n",
    "Valores calculados automaticamente a partir de los parametros:\n",
    "\n",
    "| Valor | Calculo | Resultado | trainRL | resume |\n",
    "|-------|---------|----------|---------|--------|\n",
    "| STEPS_PER_EPOCH | N_LAST_STATES_FINAL * MATCHES_PER_EPOCH | 2 * 100 = **200** | [L75](trainRL.py#L75) | [L79](trainRL_resume_latest.py#L79) |\n",
    "| ITER_PER_EPOCH | STEPS_PER_EPOCH // BATCH_SIZE | 200 // 30 = **6** | [L77](trainRL.py#L77) | [L81](trainRL_resume_latest.py#L81) |\n",
    "| REPLAY_SIZE | 100 * STEPS_PER_EPOCH | 100 * 200 = **20,000** | [L81](trainRL.py#L81) | [L85](trainRL_resume_latest.py#L85) |\n",
    "| N_BATCHS_2_UPDATE_TARGET | ITER_PER_EPOCH // 3 | 6 // 3 = **2** | [L87](trainRL.py#L87) | [L91](trainRL_resume_latest.py#L91) |\n",
    "\n",
    "### Notas:\n",
    "- **LR constante**: LR == LR_F = 5e-5 (sin cosine annealing efectivo)\n",
    "- **Sin curriculum**: N_LAST_STATES_INIT == N_LAST_STATES_FINAL = 2\n",
    "- **6 iteraciones DQN por epoca** con 100 partidas de self-play\n",
    "- **Target network se actualiza 2 veces por epoca** (cada 2 batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Bug corregido en trainRL_resume_latest.py\n",
    "\n",
    "**Problema:** Al reanudar entrenamiento, los datos historicos (win_rate, loss, q_values, epochs_results)\n",
    "NO se cargaban del archivo pickle. Esto causaba:\n",
    "1. Las graficas solo mostraban datos de la sesion actual, no el historial completo\n",
    "2. El archivo pickle se sobreescribia perdiendo TODOS los datos previos\n",
    "3. El contador `step_i` reiniciaba en -1, desalineando los epoch boundaries del loss plot\n",
    "\n",
    "**Solucion:** Se agrego bloque de restauracion en [linea 282](trainRL_resume_latest.py#L282) que:\n",
    "- Carga `epochs_results`, `loss_data`, `win_rate`, `q_values_history` del pickle existente\n",
    "- Restaura `step_i` al ultimo valor guardado para mantener continuidad\n",
    "- Log informativo con cantidad de datos restaurados\n",
    "- Manejo de errores: si falla la carga, continua con historial vacio\n",
    "\n",
    "```python\n",
    "# --- Restore historical training data when resuming ---\n",
    "if loaded_epoch > 0:\n",
    "    _pkl_path = path.join(CHECKPOINT_FOLDER, f\"{EXPERIMENT_NAME}.pkl\")\n",
    "    if path.exists(_pkl_path):\n",
    "        with open(_pkl_path, \"rb\") as f:\n",
    "            _saved = pickle.load(f)\n",
    "        epochs_results = _saved.get(\"epochs_results\", epochs_results)\n",
    "        loss_data = _saved.get(\"loss_values\", loss_data)\n",
    "        win_rate = _saved.get(\"win_rate\", win_rate)\n",
    "        q_values_history = _saved.get(\"q_values_history\", q_values_history)\n",
    "        if loss_data[\"epoch_values\"]:\n",
    "            step_i = loss_data[\"epoch_values\"][-1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Registro de entrenamientos\n\n<a id=\"train-table\"></a>\n\n| # | Fecha | Script | Epocas | Notas | Graficas | Resultado |\n|---|-------|--------|--------|-------|----------|-----------|\n| <a id=\"train-1\"></a>1 | 2026-02-15 | trainRL.py | 3500 | Test largo de 3500 epocas | [ver anexo](#anexo-1) | Se lo realizo para hacer un analisis profundo y saber cuales son sus problemas iniciales con los parametros que estan configurados por defecto |"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "## 5. Anexos - Graficas de entrenamiento\n\nImagenes en [`anexos_bitacora/`](anexos_bitacora/) con formato: `train{#}_{tipo}.png`\n\n---\n\n<a id=\"anexo-1\"></a>\n### Anexo 1 — Entrenamiento #1 ([volver a tabla](#train-1))\n\n<!-- Descomentar cuando se tengan las imagenes:\n\n#### Win Rate\n![win_rate](anexos_bitacora/train1_win_rate.png)\n\n#### Loss\n![loss](anexos_bitacora/train1_loss.png)\n\n#### Q-values\n![q_values](anexos_bitacora/train1_q_values.png)\n\n#### Boards\n![boards](anexos_bitacora/train1_boards.png)\n\n-->\n\n**Observaciones:**\n- (Pendiente)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}