{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Bitacora #5 - Combined_avg + Discount (Post-Paradoja)\n",
    "**Fecha:** 2026-02-17  \n",
    "**Autor:** Francis Bravo  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Motivacion\n",
    "\n",
    "### Contexto de entrenamientos anteriores\n",
    "\n",
    "**Bit√°cora #2 (Entrenamiento #1):**\n",
    "- Par√°metros: REWARD=\"propagate\", LOSS=\"combined_avg\", LR=5e-5, N_STATES=2\n",
    "- Resultado: ‚ùå Q_select colapsado, Win Rate ~50% vs random\n",
    "\n",
    "**Bit√°cora #3 (Entrenamiento #2):**\n",
    "- Par√°metros: REWARD=\"propagate\", LOSS=\"combined_avg\", LR=5e-4, N_STATES=6\n",
    "- Resultado: ‚ùå Q_select colapsado, Win Rate ~45% vs random\n",
    "\n",
    "**Bit√°cora #4 (Entrenamiento #1 ULTRA-AGRESIVO):**\n",
    "- Par√°metros: REWARD=\"discount\", LOSS=\"only_select\", LR=3e-4, N_STATES=8, 7 cambios simult√°neos\n",
    "- Resultado: ‚ùå Q_select colapsado, **pero descubrimiento cr√≠tico:**\n",
    "\n",
    "### üî¥ Descubrimiento: La Paradoja del Only_Select\n",
    "\n",
    "Al entrenar SOLO Q_select (`LOSS=\"only_select\"`):\n",
    "- Q_select (entrenada) ‚Üí Colaps√≥ a -1.0\n",
    "- Q_place (NO entrenada) ‚Üí **Mejor√≥ significativamente** (aprendizaje pasivo del backbone)\n",
    "\n",
    "**Implicaci√≥n:** El entrenamiento conjunto (`combined_avg`) podr√≠a **estabilizar Q_select** aprovechando se√±ales de Q_place.\n",
    "\n",
    "### Hip√≥tesis del experimento\n",
    "\n",
    "**Si combinamos `REWARD_FUNCTION=\"discount\"` (mejor se√±al temporal) con `LOSS_APPROACH=\"combined_avg\"` (entrenamiento conjunto estabilizador), entonces Q_select NO colapsar√° y ambas cabezas aprender√°n efectivamente.**\n",
    "\n",
    "**Fundamento:**\n",
    "1. `REWARD=\"discount\"` demostr√≥ reducir loss 32% (bit√°cora 4)\n",
    "2. Q_place aprendi√≥ mejor cuando el backbone se entrenaba con se√±ales de Q_select\n",
    "3. Entrenar ambas cabezas podr√≠a crear efecto estabilizador mutuo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2. Cambios de parametros\n",
    "\n",
    "### 2.1 Estrategia seleccionada\n",
    "\n",
    "**Estrategia: COMBINED_AVG + DISCOUNT (Post-Paradoja)**\n",
    "\n",
    "Esta estrategia mantiene las mejoras de la bit√°cora #4 pero revierte el cambio de `LOSS_APPROACH` bas√°ndose en la Paradoja del Only_Select.\n",
    "\n",
    "### 2.2 Cambios respecto a Bit√°cora #4 (Entrenamiento #1)\n",
    "\n",
    "| Par√°metro | Bit√°cora #4 | Bit√°cora #5 (NUEVO) | Raz√≥n del cambio |\n",
    "|-----------|-------------|---------------------|------------------|\n",
    "| **LOSS_APPROACH** | `\"only_select\"` | **`\"combined_avg\"`** | Aprovechar la Paradoja: entrenar ambas cabezas para estabilizar Q_select |\n",
    "\n",
    "**Par√°metros mantenidos de Bit√°cora #4:**\n",
    "- ‚úÖ REWARD_FUNCTION = \"discount\" (redujo loss 32%)\n",
    "- ‚úÖ LR = 3e-4 (estabiliz√≥ entrenamiento)\n",
    "- ‚úÖ GAMMA = 0.95 (necesario para discount)\n",
    "- ‚úÖ TAU = 0.01 (target network responsivo)\n",
    "- ‚úÖ N_LAST_STATES = 8 (mayor diversidad)\n",
    "- ‚úÖ TEMPERATURE_EXPLORE = 0.7 (mejor self-play)\n",
    "\n",
    "### 2.3 Resumen de cambios vs Bit√°cora #2 (baseline original)\n",
    "\n",
    "| Par√°metro | Bit√°cora #2 (Baseline) | Bit√°cora #5 (NUEVO) | Diferencia |\n",
    "|-----------|------------------------|---------------------|------------|\n",
    "| **REWARD_FUNCTION** | `\"propagate\"` | **`\"discount\"`** | Se√±al temporal clara |\n",
    "| **LOSS_APPROACH** | `\"combined_avg\"` | `\"combined_avg\"` | Sin cambio |\n",
    "| **N_LAST_STATES** | 2/2 | **8/8** | 4x m√°s diversidad |\n",
    "| **TEMPERATURE_EXPLORE** | 2.0 | **0.7** | Mejor calidad |\n",
    "| **TAU** | 0.01 | 0.01 | Sin cambio |\n",
    "| **GAMMA** | 0.99 | **0.95** | Ajustado para discount |\n",
    "| **LR** | 5e-5 | **3e-4** | 6x mayor |\n",
    "| **BATCH_SIZE** | 30 | **64** | 2x mayor |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### 2.4 Parametros completos del entrenamiento\n",
    "\n",
    "Parametros configurados para este experimento. Los valores **en negrita** son los que cambiaron respecto a la bit√°cora #2 (baseline original).\n",
    "\n",
    "#### Configuracion principal\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| STARTING_NET | `None` | [L42](trainRL.py#L42) | [L45](trainRL_resume_latest.py#L45) | Pesos aleatorios (sin checkpoint previo) |\n",
    "| EXPERIMENT_NAME | `\"07_COMBINED_DISCOUNT\"` | [L43](trainRL.py#L43) | [L46](trainRL_resume_latest.py#L46) | Nombre del experimento |\n",
    "| ARCHITECTURE | `QuartoCNN_uncoupled` | [L50](trainRL.py#L50) | [L54](trainRL_resume_latest.py#L54) | Red con cabezas independientes (mantener) |\n",
    "| LOSS_APPROACH | `\"combined_avg\"` | [L51](trainRL.py#L51) | [L55](trainRL_resume_latest.py#L55) | Promedio de Q_place y Q_select |\n",
    "| **REWARD_FUNCTION** | **`\"discount\"`** | [L52](trainRL.py#L52) | [L56](trainRL_resume_latest.py#L56) | Recompensas descontadas temporalmente |\n",
    "\n",
    "#### Hiperparametros de entrenamiento\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| EPOCHS | `3000` | [L66](trainRL.py#L66) | [L70](trainRL_resume_latest.py#L70) | Epocas totales |\n",
    "| **BATCH_SIZE** | **`64`** | [L61](trainRL.py#L61) | [L65](trainRL_resume_latest.py#L65) | Muestras por batch (2x mayor que baseline) |\n",
    "| **LR** | **`3e-4`** | [L111](trainRL.py#L111) | [L117](trainRL_resume_latest.py#L117) | Learning rate inicial (6x mayor que baseline) |\n",
    "| LR_F | `1e-5` | [L112](trainRL.py#L112) | [L118](trainRL_resume_latest.py#L118) | Learning rate final |\n",
    "| **TAU** | **`0.01`** | [L113](trainRL.py#L113) | [L119](trainRL_resume_latest.py#L119) | Tasa de soft update |\n",
    "| **GAMMA** | **`0.95`** | [L115](trainRL.py#L115) | [L121](trainRL_resume_latest.py#L121) | Factor de descuento (ajustado para discount) |\n",
    "| MAX_GRAD_NORM | `1.0` | [L110](trainRL.py#L110) | [L116](trainRL_resume_latest.py#L116) | Clipping de gradientes |\n",
    "| torch.manual_seed | `5` | [L171](trainRL.py#L171) | [L204](trainRL_resume_latest.py#L204) | Semilla para reproducibilidad |\n",
    "\n",
    "#### Generacion de experiencia\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| GEN_EXPERIENCE_BY_EPOCH | `True` | [L56](trainRL.py#L56) | [L60](trainRL_resume_latest.py#L60) | Generar experiencia nueva cada epoca |\n",
    "| MATCHES_PER_EPOCH | `100` | [L73](trainRL.py#L73) | [L77](trainRL_resume_latest.py#L77) | Partidas de self-play por epoca |\n",
    "| **N_LAST_STATES_INIT** | **`8`** | [L69](trainRL.py#L69) | [L73](trainRL_resume_latest.py#L73) | Estados del historial (inicio, 4x vs baseline) |\n",
    "| **N_LAST_STATES_FINAL** | **`8`** | [L71](trainRL.py#L71) | [L75](trainRL_resume_latest.py#L75) | Estados del historial (final) |\n",
    "| **TEMPERATURE_EXPLORE** | **`0.7`** | [L91](trainRL.py#L91) | [L95](trainRL_resume_latest.py#L95) | Temperatura para exploracion (vs 2.0 baseline) |\n",
    "| TEMPERATURE_EXPLOIT | `0.1` | [L94](trainRL.py#L94) | [L98](trainRL_resume_latest.py#L98) | Temperatura para explotacion |\n",
    "| mode_2x2 | `True` | [L63](trainRL.py#L63) | [L67](trainRL_resume_latest.py#L67) | Victoria por cuadrado 2x2 habilitada |\n",
    "\n",
    "#### Evaluacion y guardado\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| N_MATCHES_EVAL | `30` | [L59](trainRL.py#L59) | [L63](trainRL_resume_latest.py#L63) | Partidas de evaluacion vs baselines por epoca |\n",
    "| FREQ_EPOCH_SAVING | `1000` | [L96](trainRL.py#L96) | [L100](trainRL_resume_latest.py#L100) | Guardar modelo cada N epocas |\n",
    "| FREQ_EPOCH_PLOT_SHOW | `50` | [L101](trainRL.py#L101) | [L105](trainRL_resume_latest.py#L105) | Mostrar plots cada N epocas |\n",
    "| PLOTS_FOLDER | `...\\ Mech Interp\\Plots` | [L48](trainRL.py#L48) | [L52](trainRL_resume_latest.py#L52) | Carpeta de guardado de plots HTML |\n",
    "| SMOOTHING_WINDOW | `10` | [L104](trainRL.py#L104) | [L110](trainRL_resume_latest.py#L110) | Ventana de suavizado para plots |\n",
    "| Q_PLOT_TYPE | `\"hist\"` | [L107](trainRL.py#L107) | [L113](trainRL_resume_latest.py#L113) | Tipo de plot de Q-values |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 3. Valores derivados\n",
    "\n",
    "Valores calculados automaticamente con los parametros:\n",
    "\n",
    "| Valor | Calculo | Bit√°cora #2 | Bit√°cora #4 | Bit√°cora #5 |\n",
    "|-------|---------|-------------|-------------|-------------|\n",
    "| STEPS_PER_EPOCH | N_LAST_STATES_FINAL * MATCHES_PER_EPOCH | 2 * 100 = **200** | 8 * 100 = **800** | 8 * 100 = **800** |\n",
    "| ITER_PER_EPOCH | STEPS_PER_EPOCH // BATCH_SIZE | 200 // 30 = **6** | 800 // 64 = **12** | 800 // 64 = **12** |\n",
    "| REPLAY_SIZE | 100 * STEPS_PER_EPOCH | 100 * 200 = **20,000** | 100 * 800 = **80,000** | 100 * 800 = **80,000** |\n",
    "| N_BATCHS_2_UPDATE_TARGET | ITER_PER_EPOCH // 3 | 6 // 3 = **2** | 12 // 3 = **4** | 12 // 3 = **4** |\n",
    "\n",
    "### Comparaci√≥n con experimentos anteriores:\n",
    "\n",
    "**vs Bit√°cora #2 (baseline):**\n",
    "- **4x m√°s experiencia** por √©poca (800 vs 200)\n",
    "- **2x m√°s iteraciones DQN** (12 vs 6)\n",
    "- **4x m√°s replay buffer** (80k vs 20k)\n",
    "- **2x m√°s updates del target** (4 vs 2)\n",
    "\n",
    "**vs Bit√°cora #4:**\n",
    "- **Mismos valores derivados** (cambio solo en LOSS_APPROACH)\n",
    "- Permite comparaci√≥n directa del efecto de combined_avg vs only_select"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 4. Registro de entrenamientos\n",
    "\n",
    "<a id=\"train-table\"></a>\n",
    "\n",
    "| # | Fecha | Script | Epocas | Par√°metros clave | Resultado | Ver gr√°ficas |\n",
    "|---|-------|--------|--------|-----------------|-----------|--------------||\n",
    "| [1](#train-1) | 2026-02-17 | trainRL.py | 3000 | REWARD=\"discount\", LOSS=\"combined_avg\", N_STATES=8, TEMP=0.7, TAU=0.01, GAMMA=0.95, LR=3e-4 | üîÑ **EN PROGRESO** | ‚Üì |\n",
    "| [2](#train-2) | - | - | - | - | - | ‚Üì |\n",
    "| [3](#train-3) | - | - | - | - | - | ‚Üì |\n",
    "\n",
    "---\n",
    "\n",
    "### Notas sobre el entrenamiento #1\n",
    "\n",
    "**Inicio:** 2026-02-17  \n",
    "**Configuraci√≥n:** COMBINED_AVG + DISCOUNT (Post-Paradoja)  \n",
    "**Objetivo:** Aprovechar la Paradoja del Only_Select - entrenar ambas cabezas con mejor se√±al de recompensa  \n",
    "**Duraci√≥n esperada:** ~3000 √©pocas  \n",
    "**Checkpoints:** Se guardar√°n en √©poca 1000, 2000, 3000  \n",
    "\n",
    "**Hip√≥tesis a validar:**\n",
    "1. Q_select NO colapsar√° (gracias al entrenamiento conjunto estabilizador)\n",
    "2. Q_place mantendr√° o mejorar√° su diferenciaci√≥n clara\n",
    "3. Win Rate superar√° 55% vs random\n",
    "4. Win Rate vs bot_loss-BT superar√° 40%\n",
    "\n",
    "**M√©tricas cr√≠ticas a monitorear:**\n",
    "- ‚úÖ Q_select debe **mantener valores diversos** (no colapsar a -1.0)\n",
    "- ‚úÖ Q_select debe **diferenciar por recompensa** (R=-1 vs R=0 vs R=1)\n",
    "- ‚úÖ Q_place debe **mantener diferenciaci√≥n** observada en bit√°cora #4\n",
    "- ‚úÖ Win Rate debe **superar 50%** vs random de forma consistente\n",
    "- ‚úÖ Loss debe **converger** (no solo oscilar)\n",
    "\n",
    "**Comparaci√≥n clave:**\n",
    "Este entrenamiento es **casi id√©ntico** a Bit√°cora #4 excepto por `LOSS_APPROACH`. Esto permite aislar el efecto de entrenar ambas cabezas vs solo Q_select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Markdown\n",
    "from os import path\n",
    "\n",
    "IMG_FOLDER = r\"C:\\Users\\bravo\\Documents\\Metodos Numericos Pycharm\\Mech Interp\\Imagenes de la Bitacoras\"\n",
    "\n",
    "# Descomentar cuando tengas las im√°genes\n",
    "# display(Markdown(\"#### Win Rate\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"WinRate_Bitacora5_Train1.png\")))\n",
    "\n",
    "# display(Markdown(\"#### Q-values Progress\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"QValueProgress_Bitacora5_Train1.png\")))\n",
    "\n",
    "# display(Markdown(\"#### Training Loss\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"TrainingLoss_Bitacora5_Train1.png\")))\n",
    "\n",
    "# display(Markdown(\"#### Board Comparisons\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"BoardComparison_Bitacora5_Train1.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 5. Observaciones y resultados\n",
    "\n",
    "### 5.1 Entrenamiento #1 - COMBINED_AVG + DISCOUNT\n",
    "\n",
    "<a id=\"train-1\"></a>\n",
    "\n",
    "**Fecha:** 2026-02-17  \n",
    "**Status:** üîÑ EN PROGRESO\n",
    "\n",
    "#### Par√°metros usados:\n",
    "\n",
    "```python\n",
    "# Configuraci√≥n cr√≠tica (cambio vs Bit√°cora #4)\n",
    "REWARD_FUNCTION = \"discount\"      # Mantener de Bit√°cora #4\n",
    "LOSS_APPROACH = \"combined_avg\"    # CAMBIO: de only_select a combined_avg\n",
    "ARCHITECTURE = QuartoCNN_uncoupled  # Mantener\n",
    "\n",
    "# Hiperpar√°metros (mantener de Bit√°cora #4)\n",
    "LR = 3e-4\n",
    "GAMMA = 0.95\n",
    "TAU = 0.01\n",
    "N_LAST_STATES = 8\n",
    "TEMPERATURE_EXPLORE = 0.7\n",
    "BATCH_SIZE = 64\n",
    "```\n",
    "\n",
    "#### Observaciones durante el entrenamiento:\n",
    "\n",
    "**√âpoca 0-500:**\n",
    "- _completar observaciones tempranas_\n",
    "- Q_select comportamiento: _completar_\n",
    "- Q_place comportamiento: _completar_\n",
    "- Comparaci√≥n con Bit√°cora #4: _completar_\n",
    "\n",
    "**√âpoca 500-1500:**\n",
    "- _completar observaciones intermedias_\n",
    "- ¬øQ_select mantiene valores diversos?: _completar_\n",
    "- Convergencia del loss: _completar_\n",
    "\n",
    "**√âpoca 1500-3000:**\n",
    "- _completar observaciones finales_\n",
    "- Estabilidad final: _completar_\n",
    "\n",
    "#### Resultados finales (√âpoca 3000):\n",
    "\n",
    "| M√©trica | Valor | Bit√°cora #4 (only_select) | Cambio |\n",
    "|---------|-------|---------------------------|--------|\n",
    "| **Win Rate vs random** | _completar_ | ~47-50% | _completar_ |\n",
    "| **Win Rate vs bot_loss-BT** | _completar_ | ~35-37% | _completar_ |\n",
    "| **Q_select estado** | _completar_ | Colapsado a -1.0 | _completar_ |\n",
    "| **Q_select diferenciaci√≥n** | _completar_ | NO | _completar_ |\n",
    "| **Q_place diferenciaci√≥n** | _completar_ | S√ç - CLARA | _completar_ |\n",
    "| **Loss final** | _completar_ | ~0.16-0.17 | _completar_ |\n",
    "\n",
    "#### An√°lisis de gr√°ficas:\n",
    "\n",
    "**Win Rate:**\n",
    "- _completar an√°lisis_\n",
    "- Comparaci√≥n con Bit√°cora #4: _completar_\n",
    "\n",
    "**Q-values Progress:**\n",
    "- **Q_select:** _completar - ¬øcolaps√≥ o se mantuvo?_\n",
    "- **Q_place:** _completar - ¬ømantuvo diferenciaci√≥n?_\n",
    "- **Efecto del combined_avg:** _completar_\n",
    "\n",
    "**Training Loss:**\n",
    "- _completar an√°lisis de convergencia_\n",
    "- Comparaci√≥n con Bit√°cora #4: _completar_\n",
    "\n",
    "**Board Comparisons:**\n",
    "- _completar an√°lisis_\n",
    "\n",
    "#### Conclusi√≥n del entrenamiento #1:\n",
    "\n",
    "**¬øSe evit√≥ el colapso de Q_select?**\n",
    "- _completar: S√ç / NO / PARCIALMENTE_\n",
    "- Evidencia: _completar_\n",
    "\n",
    "**¬øMejor√≥ vs Bit√°cora #4?**\n",
    "- _completar comparaci√≥n_\n",
    "\n",
    "**¬øSe valid√≥ la hip√≥tesis?**\n",
    "- _completar an√°lisis_\n",
    "\n",
    "**Efecto del cambio LOSS_APPROACH:**\n",
    "- _completar - ¬øcombined_avg estabiliz√≥ Q_select?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 6. Conclusiones generales\n",
    "\n",
    "### 6.1 Comparaci√≥n con entrenamientos anteriores\n",
    "\n",
    "| M√©trica | Bit√°cora #2 | Bit√°cora #3 | Bit√°cora #4 | Bit√°cora #5 (este) |\n",
    "|---------|-------------|-------------|-------------|--------------------|\n",
    "| REWARD_FUNCTION | propagate | propagate | discount | **discount** |\n",
    "| LOSS_APPROACH | combined_avg | combined_avg | only_select | **combined_avg** |\n",
    "| Win Rate vs random | ~0.50 | ~0.45 | ~0.47-0.50 | _completar_ |\n",
    "| Win Rate vs bot_loss-BT | ~0.37 | ~0.37 | ~0.35-0.37 | _completar_ |\n",
    "| Q_select estado | Colapsado | Colapsado | Colapsado | _completar_ |\n",
    "| Q_place diferenciaci√≥n | No | D√©bil | S√≠ - CLARA | _completar_ |\n",
    "| Loss final | ~0.25 | ~0.25 | ~0.16-0.17 | _completar_ |\n",
    "\n",
    "### 6.2 Validaci√≥n/Refutaci√≥n de hip√≥tesis\n",
    "\n",
    "**Hip√≥tesis:**\n",
    "> \"Si combinamos REWARD=\"discount\" (mejor se√±al) con LOSS=\"combined_avg\" (estabilizaci√≥n), entonces Q_select NO colapsar√° y ambas cabezas aprender√°n efectivamente.\"\n",
    "\n",
    "**Resultado:** _completar: VALIDADA / REFUTADA / PARCIALMENTE VALIDADA_\n",
    "\n",
    "#### An√°lisis:\n",
    "\n",
    "_completar an√°lisis detallado_\n",
    "\n",
    "### 6.3 Efecto del cambio LOSS_APPROACH\n",
    "\n",
    "**Comparaci√≥n directa Bit√°cora #4 vs #5:**\n",
    "\n",
    "Ambos entrenamientos son id√©nticos excepto:\n",
    "- Bit√°cora #4: `LOSS=\"only_select\"`\n",
    "- Bit√°cora #5: `LOSS=\"combined_avg\"`\n",
    "\n",
    "**Diferencias observadas:**\n",
    "\n",
    "_completar tabla comparativa de m√©tricas clave_\n",
    "\n",
    "**Conclusi√≥n sobre LOSS_APPROACH:**\n",
    "\n",
    "_completar - ¬øcombined_avg es mejor que only_select?_\n",
    "\n",
    "### 6.4 Validaci√≥n de la Paradoja del Only_Select\n",
    "\n",
    "**Predicci√≥n basada en la paradoja:**\n",
    "- Con `only_select`: Q_place mejor√≥, Q_select colaps√≥\n",
    "- Con `combined_avg`: Ambas deber√≠an mejorar o al menos Q_select no colapsar\n",
    "\n",
    "**Resultado observado:**\n",
    "\n",
    "_completar - ¬øse confirm√≥ la predicci√≥n?_\n",
    "\n",
    "### 6.5 Pr√≥ximos pasos\n",
    "\n",
    "_completar bas√°ndose en los resultados obtenidos_\n",
    "\n",
    "**Si Q_select NO colaps√≥:**\n",
    "- Continuar optimizando hiperpar√°metros con esta configuraci√≥n\n",
    "- Probar entrenamientos m√°s largos (5000+ √©pocas)\n",
    "- Experimentar con shaped rewards\n",
    "\n",
    "**Si Q_select S√ç colaps√≥:**\n",
    "- Investigaci√≥n arquitectural profunda (Opci√≥n 1 de Bit√°cora #4)\n",
    "- An√°lisis de gradientes y pesos\n",
    "- Considerar cambios arquitecturales menores\n",
    "\n",
    "### 6.6 Conclusi√≥n final\n",
    "\n",
    "_completar conclusi√≥n general del experimento_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
