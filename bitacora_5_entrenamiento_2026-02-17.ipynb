{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Bitacora #5 - Combined_avg + Discount (Post-Paradoja)\n",
    "**Fecha:** 2026-02-17  \n",
    "**Autor:** Francis Bravo  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Motivacion\n",
    "\n",
    "### Contexto de entrenamientos anteriores\n",
    "\n",
    "**Bit√°cora #2 (Entrenamiento #1):**\n",
    "- Par√°metros: REWARD=\"propagate\", LOSS=\"combined_avg\", LR=5e-5, N_STATES=2\n",
    "- Resultado: ‚ùå Q_select colapsado, Win Rate ~50% vs random\n",
    "\n",
    "**Bit√°cora #3 (Entrenamiento #2):**\n",
    "- Par√°metros: REWARD=\"propagate\", LOSS=\"combined_avg\", LR=5e-4, N_STATES=6\n",
    "- Resultado: ‚ùå Q_select colapsado, Win Rate ~45% vs random\n",
    "\n",
    "**Bit√°cora #4 (Entrenamiento #1 ULTRA-AGRESIVO):**\n",
    "- Par√°metros: REWARD=\"discount\", LOSS=\"only_select\", LR=3e-4, N_STATES=8, 7 cambios simult√°neos\n",
    "- Resultado: ‚ùå Q_select colapsado, **pero descubrimiento cr√≠tico:**\n",
    "\n",
    "### üî¥ Descubrimiento: La Paradoja del Only_Select\n",
    "\n",
    "Al entrenar SOLO Q_select (`LOSS=\"only_select\"`):\n",
    "- Q_select (entrenada) ‚Üí Colaps√≥ a -1.0\n",
    "- Q_place (NO entrenada) ‚Üí **Mejor√≥ significativamente** (aprendizaje pasivo del backbone)\n",
    "\n",
    "**Implicaci√≥n:** El entrenamiento conjunto (`combined_avg`) podr√≠a **estabilizar Q_select** aprovechando se√±ales de Q_place.\n",
    "\n",
    "### Hip√≥tesis del experimento\n",
    "\n",
    "**Si combinamos `REWARD_FUNCTION=\"discount\"` (mejor se√±al temporal) con `LOSS_APPROACH=\"combined_avg\"` (entrenamiento conjunto estabilizador), entonces Q_select NO colapsar√° y ambas cabezas aprender√°n efectivamente.**\n",
    "\n",
    "**Fundamento:**\n",
    "1. `REWARD=\"discount\"` demostr√≥ reducir loss 32% (bit√°cora 4)\n",
    "2. Q_place aprendi√≥ mejor cuando el backbone se entrenaba con se√±ales de Q_select\n",
    "3. Entrenar ambas cabezas podr√≠a crear efecto estabilizador mutuo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2. Cambios de parametros\n",
    "\n",
    "### 2.1 Estrategia seleccionada\n",
    "\n",
    "**Estrategia: COMBINED_AVG + DISCOUNT (Post-Paradoja)**\n",
    "\n",
    "Esta estrategia mantiene las mejoras de la bit√°cora #4 pero revierte el cambio de `LOSS_APPROACH` bas√°ndose en la Paradoja del Only_Select.\n",
    "\n",
    "### 2.2 Cambios respecto a Bit√°cora #4 (Entrenamiento #1)\n",
    "\n",
    "| Par√°metro | Bit√°cora #4 | Bit√°cora #5 (NUEVO) | Raz√≥n del cambio |\n",
    "|-----------|-------------|---------------------|------------------|\n",
    "| **LOSS_APPROACH** | `\"only_select\"` | **`\"combined_avg\"`** | Aprovechar la Paradoja: entrenar ambas cabezas para estabilizar Q_select |\n",
    "\n",
    "**Par√°metros mantenidos de Bit√°cora #4:**\n",
    "- ‚úÖ REWARD_FUNCTION = \"discount\" (redujo loss 32%)\n",
    "- ‚úÖ LR = 3e-4 (estabiliz√≥ entrenamiento)\n",
    "- ‚úÖ GAMMA = 0.95 (necesario para discount)\n",
    "- ‚úÖ TAU = 0.01 (target network responsivo)\n",
    "- ‚úÖ N_LAST_STATES = 8 (mayor diversidad)\n",
    "- ‚úÖ TEMPERATURE_EXPLORE = 0.7 (mejor self-play)\n",
    "\n",
    "### 2.3 Resumen de cambios vs Bit√°cora #2 (baseline original)\n",
    "\n",
    "| Par√°metro | Bit√°cora #2 (Baseline) | Bit√°cora #5 (NUEVO) | Diferencia |\n",
    "|-----------|------------------------|---------------------|------------|\n",
    "| **REWARD_FUNCTION** | `\"propagate\"` | **`\"discount\"`** | Se√±al temporal clara |\n",
    "| **LOSS_APPROACH** | `\"combined_avg\"` | `\"combined_avg\"` | Sin cambio |\n",
    "| **N_LAST_STATES** | 2/2 | **8/8** | 4x m√°s diversidad |\n",
    "| **TEMPERATURE_EXPLORE** | 2.0 | **0.7** | Mejor calidad |\n",
    "| **TAU** | 0.01 | 0.01 | Sin cambio |\n",
    "| **GAMMA** | 0.99 | **0.95** | Ajustado para discount |\n",
    "| **LR** | 5e-5 | **3e-4** | 6x mayor |\n",
    "| **BATCH_SIZE** | 30 | **64** | 2x mayor |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### 2.4 Parametros completos del entrenamiento\n",
    "\n",
    "Parametros configurados para este experimento. Los valores **en negrita** son los que cambiaron respecto a la bit√°cora #2 (baseline original).\n",
    "\n",
    "#### Configuracion principal\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| STARTING_NET | `None` | [L42](trainRL.py#L42) | [L45](trainRL_resume_latest.py#L45) | Pesos aleatorios (sin checkpoint previo) |\n",
    "| EXPERIMENT_NAME | `\"07_COMBINED_DISCOUNT\"` | [L43](trainRL.py#L43) | [L46](trainRL_resume_latest.py#L46) | Nombre del experimento |\n",
    "| ARCHITECTURE | `QuartoCNN_uncoupled` | [L50](trainRL.py#L50) | [L54](trainRL_resume_latest.py#L54) | Red con cabezas independientes (mantener) |\n",
    "| LOSS_APPROACH | `\"combined_avg\"` | [L51](trainRL.py#L51) | [L55](trainRL_resume_latest.py#L55) | Promedio de Q_place y Q_select |\n",
    "| **REWARD_FUNCTION** | **`\"discount\"`** | [L52](trainRL.py#L52) | [L56](trainRL_resume_latest.py#L56) | Recompensas descontadas temporalmente |\n",
    "\n",
    "#### Hiperparametros de entrenamiento\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| EPOCHS | `3000` | [L66](trainRL.py#L66) | [L70](trainRL_resume_latest.py#L70) | Epocas totales |\n",
    "| **BATCH_SIZE** | **`64`** | [L61](trainRL.py#L61) | [L65](trainRL_resume_latest.py#L65) | Muestras por batch (2x mayor que baseline) |\n",
    "| **LR** | **`3e-4`** | [L111](trainRL.py#L111) | [L117](trainRL_resume_latest.py#L117) | Learning rate inicial (6x mayor que baseline) |\n",
    "| LR_F | `1e-5` | [L112](trainRL.py#L112) | [L118](trainRL_resume_latest.py#L118) | Learning rate final |\n",
    "| **TAU** | **`0.01`** | [L113](trainRL.py#L113) | [L119](trainRL_resume_latest.py#L119) | Tasa de soft update |\n",
    "| **GAMMA** | **`0.95`** | [L115](trainRL.py#L115) | [L121](trainRL_resume_latest.py#L121) | Factor de descuento (ajustado para discount) |\n",
    "| MAX_GRAD_NORM | `1.0` | [L110](trainRL.py#L110) | [L116](trainRL_resume_latest.py#L116) | Clipping de gradientes |\n",
    "| torch.manual_seed | `5` | [L171](trainRL.py#L171) | [L204](trainRL_resume_latest.py#L204) | Semilla para reproducibilidad |\n",
    "\n",
    "#### Generacion de experiencia\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| GEN_EXPERIENCE_BY_EPOCH | `True` | [L56](trainRL.py#L56) | [L60](trainRL_resume_latest.py#L60) | Generar experiencia nueva cada epoca |\n",
    "| MATCHES_PER_EPOCH | `100` | [L73](trainRL.py#L73) | [L77](trainRL_resume_latest.py#L77) | Partidas de self-play por epoca |\n",
    "| **N_LAST_STATES_INIT** | **`8`** | [L69](trainRL.py#L69) | [L73](trainRL_resume_latest.py#L73) | Estados del historial (inicio, 4x vs baseline) |\n",
    "| **N_LAST_STATES_FINAL** | **`8`** | [L71](trainRL.py#L71) | [L75](trainRL_resume_latest.py#L75) | Estados del historial (final) |\n",
    "| **TEMPERATURE_EXPLORE** | **`0.7`** | [L91](trainRL.py#L91) | [L95](trainRL_resume_latest.py#L95) | Temperatura para exploracion (vs 2.0 baseline) |\n",
    "| TEMPERATURE_EXPLOIT | `0.1` | [L94](trainRL.py#L94) | [L98](trainRL_resume_latest.py#L98) | Temperatura para explotacion |\n",
    "| mode_2x2 | `True` | [L63](trainRL.py#L63) | [L67](trainRL_resume_latest.py#L67) | Victoria por cuadrado 2x2 habilitada |\n",
    "\n",
    "#### Evaluacion y guardado\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| N_MATCHES_EVAL | `30` | [L59](trainRL.py#L59) | [L63](trainRL_resume_latest.py#L63) | Partidas de evaluacion vs baselines por epoca |\n",
    "| FREQ_EPOCH_SAVING | `1000` | [L96](trainRL.py#L96) | [L100](trainRL_resume_latest.py#L100) | Guardar modelo cada N epocas |\n",
    "| FREQ_EPOCH_PLOT_SHOW | `50` | [L101](trainRL.py#L101) | [L105](trainRL_resume_latest.py#L105) | Mostrar plots cada N epocas |\n",
    "| PLOTS_FOLDER | `...\\ Mech Interp\\Plots` | [L48](trainRL.py#L48) | [L52](trainRL_resume_latest.py#L52) | Carpeta de guardado de plots HTML |\n",
    "| SMOOTHING_WINDOW | `10` | [L104](trainRL.py#L104) | [L110](trainRL_resume_latest.py#L110) | Ventana de suavizado para plots |\n",
    "| Q_PLOT_TYPE | `\"hist\"` | [L107](trainRL.py#L107) | [L113](trainRL_resume_latest.py#L113) | Tipo de plot de Q-values |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 3. Valores derivados\n",
    "\n",
    "Valores calculados automaticamente con los parametros:\n",
    "\n",
    "| Valor | Calculo | Bit√°cora #2 | Bit√°cora #4 | Bit√°cora #5 |\n",
    "|-------|---------|-------------|-------------|-------------|\n",
    "| STEPS_PER_EPOCH | N_LAST_STATES_FINAL * MATCHES_PER_EPOCH | 2 * 100 = **200** | 8 * 100 = **800** | 8 * 100 = **800** |\n",
    "| ITER_PER_EPOCH | STEPS_PER_EPOCH // BATCH_SIZE | 200 // 30 = **6** | 800 // 64 = **12** | 800 // 64 = **12** |\n",
    "| REPLAY_SIZE | 100 * STEPS_PER_EPOCH | 100 * 200 = **20,000** | 100 * 800 = **80,000** | 100 * 800 = **80,000** |\n",
    "| N_BATCHS_2_UPDATE_TARGET | ITER_PER_EPOCH // 3 | 6 // 3 = **2** | 12 // 3 = **4** | 12 // 3 = **4** |\n",
    "\n",
    "### Comparaci√≥n con experimentos anteriores:\n",
    "\n",
    "**vs Bit√°cora #2 (baseline):**\n",
    "- **4x m√°s experiencia** por √©poca (800 vs 200)\n",
    "- **2x m√°s iteraciones DQN** (12 vs 6)\n",
    "- **4x m√°s replay buffer** (80k vs 20k)\n",
    "- **2x m√°s updates del target** (4 vs 2)\n",
    "\n",
    "**vs Bit√°cora #4:**\n",
    "- **Mismos valores derivados** (cambio solo en LOSS_APPROACH)\n",
    "- Permite comparaci√≥n directa del efecto de combined_avg vs only_select"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## 4. Registro de entrenamientos\n\n<a id=\"train-table\"></a>\n\n| # | Fecha | Script | Epocas | Par√°metros clave | Resultado | Ver gr√°ficas |\n|---|-------|--------|--------|-----------------|-----------|--------------|\n| [1](#train-1) | 2026-02-17 | trainRL.py | 3000 | REWARD=\"discount\", LOSS=\"combined_avg\", N_STATES=8, TEMP=0.7, TAU=0.01, GAMMA=0.95, LR=3e-4 | ‚ùå **FALLIDO** - Doble colapso: Q_place‚Üí+1.0, Q_select‚Üí-1.0 | ‚Üì |\n| [2](#train-2) | - | - | - | - | - | ‚Üì |\n| [3](#train-3) | - | - | - | - | - | ‚Üì |\n\n---\n\n### Resumen del entrenamiento #1\n\n**Completado:** 2026-02-17  \n**Hip√≥tesis:** Combined_avg estabilizar√≠a Q_select usando la se√±al mutua de ambas cabezas  \n**Resultado:** ‚ùå **FALLIDO - Nuevo fen√≥meno descubierto: DOBLE COLAPSO**\n\n**M√©tricas clave:**\n- ‚úÖ Loss: ~0.10-0.11 (mejor que bit√°cora #4, -35% vs ~0.16)\n- ‚ùå Q_place: **Colapsado a +1.0** desde √©poca ~100 (NUEVO - opuesto al de Q_select)\n- ‚ùå Q_select: **Colapsado a -1.0** (igual que siempre)\n- ‚û°Ô∏è Win Rate vs random: ~47-55% (sin mejora significativa)\n- ‚û°Ô∏è Win Rate vs bot_loss-BT: ~35-40% (sin mejora)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Markdown\n",
    "from os import path\n",
    "\n",
    "IMG_FOLDER = r\"C:\\Users\\bravo\\Documents\\Metodos Numericos Pycharm\\Mech Interp\\Imagenes de la Bitacoras\"\n",
    "\n",
    "# Descomentar cuando tengas las im√°genes\n",
    "# display(Markdown(\"#### Win Rate\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"WinRate_Bitacora5_Train1.png\")))\n",
    "\n",
    "# display(Markdown(\"#### Q-values Progress\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"QValueProgress_Bitacora5_Train1.png\")))\n",
    "\n",
    "# display(Markdown(\"#### Training Loss\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"TrainingLoss_Bitacora5_Train1.png\")))\n",
    "\n",
    "# display(Markdown(\"#### Board Comparisons\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"BoardComparison_Bitacora5_Train1.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## 5. Observaciones y resultados\n\n### 5.1 Entrenamiento #1 - COMBINED_AVG + DISCOUNT\n\n<a id=\"train-1\"></a>\n\n**Fecha:** 2026-02-17  \n**Status:** ‚úÖ COMPLETADO\n\n#### Par√°metros usados:\n\n```python\nREWARD_FUNCTION = \"discount\"\nLOSS_APPROACH = \"combined_avg\"    # Cambio vs Bit√°cora #4 (era only_select)\nARCHITECTURE = QuartoCNN_uncoupled\nLR = 3e-4\nGAMMA = 0.95\nTAU = 0.01\nN_LAST_STATES = 8\nTEMPERATURE_EXPLORE = 0.7\nBATCH_SIZE = 64\n```\n\n#### Resultados finales (√âpoca 3000):\n\n| M√©trica | Valor observado | Bit√°cora #4 (only_select) | Cambio |\n|---------|----------------|---------------------------|--------|\n| **Win Rate vs random** | ~47-55% | ~47-50% | ‚û°Ô∏è Sin mejora |\n| **Win Rate vs bot_loss-BT** | ~35-40% | ~35-37% | ‚û°Ô∏è Sin mejora |\n| **Q_place estado** | **Colapsado a +1.0** ‚ö†Ô∏è NUEVO | Diferenciaci√≥n clara | ‚ùå Empeor√≥ |\n| **Q_place diferenciaci√≥n** | **NO** | S√ç | ‚ùå Empeor√≥ |\n| **Q_select estado** | **Colapsado a -1.0** | Colapsado a -1.0 | ‚ùå Sin cambio |\n| **Q_select diferenciaci√≥n** | **NO** | NO | ‚ùå Sin cambio |\n| **Loss final** | ~0.10-0.11 | ~0.16-0.17 | ‚úÖ -35% mejor |\n\n#### An√°lisis detallado por gr√°fica:\n\n##### 1. Training Loss\n- **Observaci√≥n:** Comienza en ~0.17 y cae abruptamente a ~0.10-0.11 en las primeras ~100 √©pocas\n- **Luego:** Se mantiene estable oscilando entre 0.09-0.12 hasta √©poca 3000\n- **Comparaci√≥n:** 35% m√°s bajo que Bit√°cora #4 (~0.16) y 56% m√°s bajo que Bit√°coras #2/#3 (~0.25)\n- **‚ö†Ô∏è Interpretaci√≥n cr√≠tica:** La ca√≠da abrupta inicial NO indica aprendizaje real ‚Äî indica que ambas cabezas colapsaron r√°pidamente hacia sus extremos (+1 y -1), minimizando el MSE de forma artificial\n- La oscilaci√≥n posterior es ruido residual de un modelo ya colapsado\n\n##### 2. Win Rate vs Rivals\n- **vs bot_random (naranja):** Oscila entre 45-55%, media ~50%\n  - Ligeramente m√°s variable que Bit√°cora #4\n  - Sin mejora sostenida ‚Äî sigue en nivel aleatorio\n- **vs bot_loss-BT (azul):** Oscila entre 30-40%, media ~37%\n  - Sin cambio vs Bit√°coras anteriores\n- **Conclusi√≥n:** A pesar de un loss 35% menor, el gameplay no mejor√≥ ‚Äî confirma que el loss bajo es artificial (colapso), no aprendizaje real\n\n##### 3. Q-value Progress ‚ö†Ô∏è **NUEVO HALLAZGO CR√çTICO: DOBLE COLAPSO**\n\n**Q_place (fila superior) ‚Äî NUEVO FEN√ìMENO:**\n- Las 3 gr√°ficas (R=-1, R=0, R=1) muestran **exactamente el mismo patr√≥n**\n- L√≠nea amarilla brillante en la parte SUPERIOR (Q=+1.0) desde √©poca ~0-100\n- Todo lo dem√°s completamente oscuro (0%)\n- **Q_place colaps√≥ a +1.0 en todas las condiciones de recompensa**\n- ‚ùå **Sin diferenciaci√≥n por recompensa** ‚Äî R=-1, R=0, R=1 son id√©nticos\n- **OPUESTO a Q_select:** Q_place satura en +1.0 mientras Q_select colapsa en -1.0\n\n**Q_select (fila inferior) ‚Äî MISMO COLAPSO QUE SIEMPRE:**\n- Las 3 gr√°ficas muestran l√≠nea amarilla en la parte INFERIOR (Q=-1.0)\n- Colapsa a -1.0 desde el inicio (incluso m√°s r√°pido que Bit√°coras anteriores)\n- ‚ùå Sin diferenciaci√≥n por recompensa\n\n**Patr√≥n observado de colapso dual:**\n```\nQ_place ‚Üí saturado en +1.0  (cabeza \"optimista\" ‚Äî todo parece victoria)\nQ_select ‚Üí colapsado en -1.0 (cabeza \"pesimista\" ‚Äî todo parece derrota)\n```\n\n##### 4. Board Comparisons\n- ‚úÖ Bug de visualizaci√≥n corregido ‚Äî tableros muestran piezas correctamente\n- Se observan estados tard√≠os del juego (12-15 piezas en tablero)\n- Primer tablero: \"27|placed|Player 1 | R=1.00\" ‚Äî estado ganador real\n- Los tableros pre/post acci√≥n muestran diferencias m√≠nimas entre acci√≥n del modelo y alternativas\n- **Observaci√≥n:** Las diferencias entre fila superior e inferior son muy peque√±as, sugiriendo que el modelo no est√° eligiendo acciones muy distintas a las alternativas (pol√≠tica casi aleatoria)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## 6. Conclusiones generales\n\n### 6.1 Comparaci√≥n con todos los entrenamientos\n\n| M√©trica | Bit. #2 | Bit. #3 | Bit. #4 (only_select) | **Bit. #5 (combined_avg)** |\n|---------|---------|---------|----------------------|---------------------------|\n| REWARD | propagate | propagate | discount | **discount** |\n| LOSS | combined_avg | combined_avg | only_select | **combined_avg** |\n| Win Rate vs random | ~0.50 | ~0.45 | ~0.47-0.50 | **~0.47-0.55** |\n| Win Rate vs bot_loss-BT | ~0.37 | ~0.37 | ~0.35-0.37 | **~0.35-0.40** |\n| Q_place estado | Sin diferen. | D√©bil | ‚úÖ Diferenciaci√≥n | ‚ùå **Colapsado +1.0** |\n| Q_select estado | Colapsado -1.0 | Colapsado -1.0 | Colapsado -1.0 | **Colapsado -1.0** |\n| Loss final | ~0.25 | ~0.25 | ~0.16 | **~0.10-0.11** |\n| Loss real vs artificial | Real | Real | Real | **‚ö†Ô∏è ARTIFICIAL** |\n\n### 6.2 Validaci√≥n de la hip√≥tesis\n\n**Hip√≥tesis:**\n> \"Si combinamos REWARD=\"discount\" con LOSS=\"combined_avg\", Q_select NO colapsar√° y ambas cabezas aprender√°n efectivamente.\"\n\n**Resultado: ‚ùå HIP√ìTESIS REFUTADA ‚Äî Resultados peores que Bit√°cora #4**\n\n1. **¬øLOSS=\"combined_avg\" estabiliz√≥ Q_select?**\n   - ‚ùå NO ‚Äî Q_select colaps√≥ igual a -1.0, incluso m√°s r√°pido\n\n2. **¬øQ_place mantuvo su diferenciaci√≥n?**\n   - ‚ùå NO ‚Äî Q_place, que en Bit√°cora #4 diferenciaba perfectamente, ahora colaps√≥ a +1.0\n\n3. **¬øMejor√≥ el Win Rate?**\n   - ‚ùå NO ‚Äî Sin diferencia significativa\n\n### 6.3 Descubrimiento principal: El Doble Colapso con Polarizaci√≥n Opuesta\n\n**Hallazgo m√°s importante:**\n\nCon `LOSS=\"combined_avg\"` entrenando ambas cabezas simult√°neamente, la red se polariz√≥:\n\n```\nQ_place  ‚Üí +1.0  (colapso \"optimista\": predice victoria en todo estado)\nQ_select ‚Üí -1.0  (colapso \"pesimista\": predice derrota en todo estado)\n```\n\n**¬øPor qu√© ocurre esto?**\n\nEl entrenamiento conjunto con `combined_avg` crea una **din√°mica adversarial impl√≠cita**:\n\n1. Q_place recibe gradientes que la empujan hacia +1.0 (predominan las victorias de self-play donde el jugador activo suele ganar m√°s a menudo que perder en los √∫ltimos estados)\n2. Q_select recibe gradientes que la empujan hacia -1.0 (los estados de selecci√≥n son m√°s ambiguos y la se√±al de descuento llega m√°s d√©bil)\n3. Al promediar los losses, la p√©rdida total se minimiza cuando cada cabeza va a su extremo opuesto\n4. El loss artificial de ~0.10-0.11 resulta de que el MSE entre predicciones extremas (+1 y -1) y los targets descontados es bajo de forma espuria\n\n**Implicaci√≥n:** `combined_avg` NO es el mecanismo estabilizador esperado ‚Äî es la causa del doble colapso.\n\n### 6.4 S√≠ntesis de 5 bit√°coras: Mapa de colapsos\n\n| Bit√°cora | LOSS | Q_place | Q_select |\n|----------|------|---------|---------|\n| #2 | combined_avg + propagate | Sin diferenciaci√≥n | Colapsado -1.0 |\n| #3 | combined_avg + propagate | D√©bil | Colapsado -1.0 |\n| #4 | **only_select** + discount | ‚úÖ Diferenciaci√≥n CLARA | Colapsado -1.0 |\n| #5 | **combined_avg** + discount | ‚ùå Colapsado **+1.0** | Colapsado -1.0 |\n\n**Patr√≥n emergente:**\n- `combined_avg` ‚Üí Q_place tiende a saturar en +1.0, Q_select en -1.0\n- `only_select` ‚Üí Q_select sigue colapsando pero Q_place mejora pasivamente\n- **Q_select colapsa a -1.0 en los 4 entrenamientos sin excepci√≥n**\n- **El √∫nico entrenamiento con Q_place √∫til fue Bit√°cora #4 (only_select)**\n\n### 6.5 Confirmaci√≥n: El problema es arquitectural y estructural\n\nDespu√©s de 4 entrenamientos distintos, la evidencia es concluyente:\n\n**Ning√∫n cambio de hiperpar√°metros resuelve el colapso de Q_select:**\n\n| Cambio intentado | Resultado en Q_select |\n|-----------------|----------------------|\n| LR: 5e-5 ‚Üí 5e-4 ‚Üí 3e-4 | Colapsado |\n| GAMMA: 0.99 ‚Üí 0.90 ‚Üí 0.95 | Colapsado |\n| TAU: 0.01 ‚Üí 0.005 ‚Üí 0.01 | Colapsado |\n| N_LAST_STATES: 2 ‚Üí 6 ‚Üí 8 | Colapsado |\n| REWARD: propagate ‚Üí discount | Colapsado |\n| LOSS: combined_avg ‚Üí only_select ‚Üí combined_avg | Colapsado |\n\n**Diagn√≥stico final:** El colapso de Q_select es **independiente de todos los hiperpar√°metros probados**. Esto descarta causas de entrenamiento y confirma un **problema en la arquitectura o inicializaci√≥n de la cabeza Q_select**.\n\n### 6.6 Pr√≥ximos pasos recomendados\n\n#### Acci√≥n prioritaria: Inspecci√≥n arquitectural de la cabeza Q_select üî¨\n\nAntes de cualquier nuevo entrenamiento de hiperpar√°metros, se deben investigar:\n\n**1. An√°lisis de gradientes:**\n- Registrar la norma de los gradientes de Q_select por √©poca\n- Verificar si los gradientes son nulos (dead neurons) o explosivos desde el inicio\n- Comparar con gradientes de Q_place\n\n**2. An√°lisis de pesos:**\n- Cargar checkpoint de √©poca 50 (antes del colapso) y √©poca 200 (despu√©s)\n- Comparar distribuci√≥n de pesos de la capa final de Q_select\n- Identificar si hay neuronas con pesos saturados\n\n**3. Probar `LOSS=\"only_place\"`:**\n- Si `only_select` mejor√≥ Q_place, entonces `only_place` podr√≠a revelar si Q_place puede resolver el problema sola\n- Permite aislar completamente la din√°mica de Q_place sin interferencia de Q_select\n\n**4. Ajustar activaci√≥n final de Q_select:**\n- Si usa `tanh` ‚Üí Probar sin activaci√≥n (Q-values sin l√≠mite)\n- Si usa `ReLU` ‚Üí Dead ReLU posible, probar `LeakyReLU`\n- Verificar que la funci√≥n de activaci√≥n sea compatible con Q-values negativos\n\n### 6.7 Conclusi√≥n final\n\n**Estado del proyecto tras 5 bit√°coras:**\n- ‚ùå Q_select colapsa de forma estructural e inevitable\n- ‚ùå El win rate no supera el nivel aleatorio en ning√∫n entrenamiento\n- ‚úÖ Identificamos que `only_select` da mejores resultados que `combined_avg` para Q_place\n- ‚úÖ Mapeamos el comportamiento de colapso en m√∫ltiples condiciones\n- ‚úÖ El loss bajo de Bit√°cora #5 es artificial (colapso dual), no aprendizaje real\n\n**Lecci√≥n cr√≠tica:**\n> Un loss bajo NO garantiza aprendizaje. En este caso, el loss de ~0.10 es consecuencia del colapso dual de ambas cabezas a sus extremos, no de haber aprendido la pol√≠tica √≥ptima.\n\n**Diagn√≥stico definitivo:**\n> El sistema no puede aprender con la arquitectura y se√±ales actuales. La siguiente acci√≥n debe ser **investigaci√≥n arquitectural**, no m√°s cambios de hiperpar√°metros."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}