{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Bitacora #4 - Experimento con REWARD_FUNCTION y LOSS_APPROACH\n",
    "**Fecha:** 2026-02-16  \n",
    "**Autor:** Francis Bravo  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Motivacion\n",
    "\n",
    "En los entrenamientos anteriores ([#1](bitacora_2_entrenamiento_2026-02-15.ipynb#train-1) y [#2](bitacora_3_entrenamiento_2026-02-16.ipynb#train-2)) se observ√≥:\n",
    "\n",
    "- **Win Rate estancado** en ~40-45% vs random (rendimiento aleatorio)\n",
    "- **Q_select colapsado** a -1.0 sin importar la recompensa (cabeza muerta)\n",
    "- **Q_place sin diferenciacion** clara entre victorias y derrotas\n",
    "- **Loss estancado** en ~0.25 sin mejora sostenida\n",
    "\n",
    "### An√°lisis de las causas ra√≠z\n",
    "\n",
    "Tras [corregir el bug de visualizaci√≥n](bitacora_3_entrenamiento_2026-02-16.ipynb#cell-7), se confirm√≥ que:\n",
    "\n",
    "1. **Los datos de entrenamiento son correctos** (bug solo afectaba visualizaci√≥n)\n",
    "2. **El problema NO es por datos corruptos**\n",
    "3. **Las causas reales son arquitecturales/algor√≠tmicas:**\n",
    "   - `REWARD_FUNCTION=\"propagate\"` ‚Üí Se√±al de aprendizaje d√©bil (todos los estados reciben la misma recompensa)\n",
    "   - `LOSS_APPROACH=\"combined_avg\"` ‚Üí Promedia Q_place y Q_select, posiblemente diluyendo la se√±al\n",
    "   - Posible problema en arquitectura `QuartoCNN_uncoupled`\n",
    "\n",
    "### Hip√≥tesis del experimento\n",
    "\n",
    "**Si cambiamos `REWARD_FUNCTION` y `LOSS_APPROACH` para dar se√±ales de aprendizaje m√°s fuertes y enfocadas, entonces Q_select dejar√° de colapsar y el modelo aprender√° pol√≠ticas efectivas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 2. Cambios de parametros\n",
    "\n",
    "### 2.1 Propuesta seleccionada\n",
    "\n",
    "**Propuesta: ULTRA-AGRESIVA (Mezcla de todos los cambios recomendados)**\n",
    "\n",
    "Esta propuesta combina m√∫ltiples cambios simult√°neos para maximizar la diferencia con los entrenamientos anteriores y atacar todas las causas ra√≠z identificadas.\n",
    "\n",
    "**NOTA:** Se mantiene la arquitectura `QuartoCNN_uncoupled` por restricci√≥n del proyecto.\n",
    "\n",
    "### 2.2 Resumen de cambios vs entrenamiento #2\n",
    "\n",
    "| Parametro | Entrenamiento #2 | Nuevo valor | Razon |\n",
    "|-----------|------------------|-------------|-------|\n",
    "| **REWARD_FUNCTION** | `\"propagate\"` | **`\"discount\"`** | Recompensas descontadas temporalmente = se√±al de aprendizaje m√°s fuerte con credit assignment claro |\n",
    "| **LOSS_APPROACH** | `\"combined_avg\"` | **`\"only_select\"`** | Enfocarse exclusivamente en rescatar la cabeza Q_select colapsada |\n",
    "| **N_LAST_STATES** | 6/6 | **8/8** | A√∫n m√°s diversidad de estados en el historial |\n",
    "| **TEMPERATURE_EXPLORE** | 1.0 | **0.7** | Menos exploraci√≥n aleatoria, m√°s explotaci√≥n de pol√≠ticas aprendidas |\n",
    "| **TAU** | 0.005 | **0.01** | Target network se actualiza 2x m√°s r√°pido (m√°s responsivo) |\n",
    "| **GAMMA** | 0.90 | **0.95** | Mayor peso a recompensas futuras (necesario para \"discount\") |\n",
    "| **LR** | 5e-4 | **3e-4** | Ligeramente m√°s conservador para estabilidad con tantos cambios |\n",
    "\n",
    "**Cambios totales: 7 par√°metros modificados simult√°neamente**\n",
    "\n",
    "### 2.3 Justificaci√≥n de la estrategia\n",
    "\n",
    "Esta propuesta ataca **simult√°neamente** las causas ra√≠z identificadas:\n",
    "\n",
    "1. **Se√±al de recompensa d√©bil** ‚Üí `REWARD_FUNCTION=\"discount\"` + `GAMMA=0.95`\n",
    "2. **Diluci√≥n de la se√±al de aprendizaje** ‚Üí `LOSS_APPROACH=\"only_select\"`\n",
    "3. **Cabeza Q_select colapsada** ‚Üí Entrenamiento exclusivo con `only_select`\n",
    "\n",
    "Adem√°s optimiza:\n",
    "- **Diversidad de datos** ‚Üí `N_LAST_STATES=8`\n",
    "- **Calidad de exploraci√≥n** ‚Üí `TEMPERATURE_EXPLORE=0.7`\n",
    "- **Velocidad de convergencia** ‚Üí `TAU=0.01`\n",
    "\n",
    "**Arquitectura mantenida:** `QuartoCNN_uncoupled` (restricci√≥n del proyecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### 2.4 Parametros completos del entrenamiento\n",
    "\n",
    "Parametros configurados para este experimento. Los valores **en negrita** son los que cambiaron respecto al entrenamiento #2.\n",
    "\n",
    "#### Configuracion principal\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| STARTING_NET | `None` | [L42](trainRL.py#L42) | [L45](trainRL_resume_latest.py#L45) | Pesos aleatorios (sin checkpoint previo) |\n",
    "| EXPERIMENT_NAME | `\"06_ULTRA\"` | [L43](trainRL.py#L43) | [L46](trainRL_resume_latest.py#L46) | Nombre del experimento |\n",
    "| ARCHITECTURE | `QuartoCNN_uncoupled` | [L50](trainRL.py#L50) | [L54](trainRL_resume_latest.py#L54) | Red con cabezas independientes (mantener) |\n",
    "| **LOSS_APPROACH** | **`\"only_select\"`** | [L51](trainRL.py#L51) | [L55](trainRL_resume_latest.py#L55) | Solo entrena cabeza Q_select |\n",
    "| **REWARD_FUNCTION** | **`\"discount\"`** | [L52](trainRL.py#L52) | [L56](trainRL_resume_latest.py#L56) | Recompensas descontadas temporalmente |\n",
    "\n",
    "#### Hiperparametros de entrenamiento\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| EPOCHS | `3000` | [L66](trainRL.py#L66) | [L70](trainRL_resume_latest.py#L70) | Epocas totales |\n",
    "| BATCH_SIZE | `64` | [L61](trainRL.py#L61) | [L65](trainRL_resume_latest.py#L65) | Muestras por batch (mantener) |\n",
    "| **LR** | **`3e-4`** | [L111](trainRL.py#L111) | [L117](trainRL_resume_latest.py#L117) | Learning rate inicial (40% menor) |\n",
    "| LR_F | `1e-5` | [L112](trainRL.py#L112) | [L118](trainRL_resume_latest.py#L118) | Learning rate final (mantener) |\n",
    "| **TAU** | **`0.01`** | [L113](trainRL.py#L113) | [L119](trainRL_resume_latest.py#L119) | Tasa de soft update (2x m√°s r√°pido) |\n",
    "| **GAMMA** | **`0.95`** | [L115](trainRL.py#L115) | [L121](trainRL_resume_latest.py#L121) | Factor de descuento (mayor para discount) |\n",
    "| MAX_GRAD_NORM | `1.0` | [L110](trainRL.py#L110) | [L116](trainRL_resume_latest.py#L116) | Clipping de gradientes (mantener) |\n",
    "| torch.manual_seed | `5` | [L171](trainRL.py#L171) | [L204](trainRL_resume_latest.py#L204) | Semilla para reproducibilidad |\n",
    "\n",
    "#### Generacion de experiencia\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| GEN_EXPERIENCE_BY_EPOCH | `True` | [L56](trainRL.py#L56) | [L60](trainRL_resume_latest.py#L60) | Generar experiencia nueva cada epoca |\n",
    "| MATCHES_PER_EPOCH | `100` | [L73](trainRL.py#L73) | [L77](trainRL_resume_latest.py#L77) | Partidas de self-play por epoca |\n",
    "| **N_LAST_STATES_INIT** | **`8`** | [L69](trainRL.py#L69) | [L73](trainRL_resume_latest.py#L73) | Estados del historial (inicio, +33%) |\n",
    "| **N_LAST_STATES_FINAL** | **`8`** | [L71](trainRL.py#L71) | [L75](trainRL_resume_latest.py#L75) | Estados del historial (final, sin curriculum) |\n",
    "| **TEMPERATURE_EXPLORE** | **`0.7`** | [L91](trainRL.py#L91) | [L95](trainRL_resume_latest.py#L95) | Temperatura para exploracion (30% menos random) |\n",
    "| TEMPERATURE_EXPLOIT | `0.1` | [L94](trainRL.py#L94) | [L98](trainRL_resume_latest.py#L98) | Temperatura para explotacion (mantener) |\n",
    "| mode_2x2 | `True` | [L63](trainRL.py#L63) | [L67](trainRL_resume_latest.py#L67) | Victoria por cuadrado 2x2 habilitada |\n",
    "\n",
    "#### Evaluacion y guardado\n",
    "\n",
    "| Parametro | Valor | trainRL | resume | Descripcion |\n",
    "|-----------|-------|---------|--------|-------------|\n",
    "| N_MATCHES_EVAL | `30` | [L59](trainRL.py#L59) | [L63](trainRL_resume_latest.py#L63) | Partidas de evaluacion vs baselines por epoca |\n",
    "| FREQ_EPOCH_SAVING | `1000` | [L96](trainRL.py#L96) | [L100](trainRL_resume_latest.py#L100) | Guardar modelo cada N epocas |\n",
    "| FREQ_EPOCH_PLOT_SHOW | `50` | [L101](trainRL.py#L101) | [L105](trainRL_resume_latest.py#L105) | Mostrar plots cada N epocas |\n",
    "| PLOTS_FOLDER | `...\\Mech Interp\\Plots` | [L48](trainRL.py#L48) | [L52](trainRL_resume_latest.py#L52) | Carpeta de guardado de plots HTML |\n",
    "| SMOOTHING_WINDOW | `10` | [L104](trainRL.py#L104) | [L110](trainRL_resume_latest.py#L110) | Ventana de suavizado para plots |\n",
    "| Q_PLOT_TYPE | `\"hist\"` | [L107](trainRL.py#L107) | [L113](trainRL_resume_latest.py#L113) | Tipo de plot de Q-values |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 3. Valores derivados\n",
    "\n",
    "Valores calculados automaticamente con los nuevos parametros:\n",
    "\n",
    "| Valor | Calculo | Entrenamiento #2 | Nuevo valor |\n",
    "|-------|---------|------------------|-------------|\n",
    "| STEPS_PER_EPOCH | N_LAST_STATES_FINAL * MATCHES_PER_EPOCH | 6 * 100 = **600** | 8 * 100 = **800** |\n",
    "| ITER_PER_EPOCH | STEPS_PER_EPOCH // BATCH_SIZE | 600 // 64 = **9** | 800 // 64 = **12** |\n",
    "| REPLAY_SIZE | 100 * STEPS_PER_EPOCH | 100 * 600 = **60,000** | 100 * 800 = **80,000** |\n",
    "| N_BATCHS_2_UPDATE_TARGET | ITER_PER_EPOCH // 3 | 9 // 3 = **3** | 12 // 3 = **4** |\n",
    "\n",
    "### Impacto de los cambios derivados:\n",
    "\n",
    "- **33% m√°s experiencia por √©poca** (800 vs 600 transiciones)\n",
    "- **33% m√°s iteraciones DQN** por √©poca (12 vs 9)\n",
    "- **33% m√°s replay buffer** (80k vs 60k transiciones)\n",
    "- **Target network se actualiza 4 veces por √©poca** (vs 3 antes)\n",
    "- **Combinado con TAU=0.01 (2x m√°s r√°pido)** ‚Üí Target network much√≠simo m√°s responsivo\n",
    "\n",
    "### Diferencia total vs entrenamiento #1 (baseline):\n",
    "\n",
    "| M√©trica | Entrenamiento #1 | Este experimento | Mejora |\n",
    "|---------|------------------|------------------|--------|\n",
    "| STEPS_PER_EPOCH | 200 | 800 | **4x** |\n",
    "| ITER_PER_EPOCH | 6 | 12 | **2x** |\n",
    "| REPLAY_SIZE | 20,000 | 80,000 | **4x** |\n",
    "| Target updates/√©poca | 2 | 4 | **2x** |\n",
    "| Velocidad de update (TAU) | 0.01 | 0.01 | **1x** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 4. Registro de entrenamientos\n",
    "\n",
    "<a id=\"train-table\"></a>\n",
    "\n",
    "| # | Fecha | Script | Epocas | Par√°metros clave | Resultado | Ver gr√°ficas |\n",
    "|---|-------|--------|--------|-----------------|-----------|--------------|\n",
    "| [1](#train-1) | 2026-02-16 | trainRL.py | 3000 | REWARD=\"discount\", LOSS=\"only_select\", N_STATES=8, TEMP=0.7, TAU=0.01, GAMMA=0.95, LR=3e-4 | ‚ùå **FALLIDO** - Q_select colaps√≥, Paradoja descubierta | ‚Üì |\n",
    "| [2](#train-2) | - | - | - | - | - | ‚Üì |\n",
    "| [3](#train-3) | - | - | - | - | - | ‚Üì |\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen del entrenamiento #1\n",
    "\n",
    "**Completado:** 2026-02-16  \n",
    "**Configuraci√≥n:** ULTRA-AGRESIVA (7 par√°metros modificados)  \n",
    "**Objetivo:** Rescatar Q_select con REWARD=\"discount\" + LOSS=\"only_select\"  \n",
    "**Duraci√≥n:** 3000 √©pocas completadas  \n",
    "\n",
    "**Resultado final:** ‚ùå **FALLIDO**\n",
    "- Q_select colaps√≥ igual que en entrenamientos anteriores\n",
    "- Win Rate sin mejora significativa (~47-50% vs random)\n",
    "- Loss mejor√≥ 32% pero sin traducirse en mejor gameplay\n",
    "\n",
    "**Hallazgo cr√≠tico:** üî¨ **PARADOJA DEL ONLY_SELECT**\n",
    "- Q_select (entrenada) ‚Üí Colaps√≥ completamente\n",
    "- Q_place (NO entrenada) ‚Üí Mejor√≥ significativamente\n",
    "- **Conclusi√≥n:** El problema es arquitectural, no de hiperpar√°metros\n",
    "\n",
    "**M√©tricas clave:**\n",
    "- ‚úÖ Loss: ~0.16 (vs ~0.25 anterior, -32% mejor)\n",
    "- ‚úÖ Q_place: Diferenciaci√≥n clara por recompensa\n",
    "- ‚ùå Q_select: Colapsado a -1.0 despu√©s de √©poca 1000\n",
    "- ‚ùå Win Rate vs random: ~47-50% (nivel aleatorio)\n",
    "- ‚ùå Win Rate vs bot_loss-BT: ~35-37% (sin mejora)\n",
    "\n",
    "**Recomendaci√≥n:** Investigaci√≥n arquitectural de Q_select antes de m√°s entrenamientos de hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Markdown\n",
    "from os import path\n",
    "\n",
    "IMG_FOLDER = r\"C:\\Users\\bravo\\Documents\\Metodos Numericos Pycharm\\Mech Interp\\Imagenes de la Bitacoras\"\n",
    "\n",
    "# Descomentar cuando tengas las im√°genes\n",
    "# display(Markdown(\"#### Win Rate\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"nombre_imagen.png\")))\n",
    "\n",
    "# display(Markdown(\"#### Q-values Progress\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"nombre_imagen.png\")))\n",
    "\n",
    "# display(Markdown(\"#### Training Loss\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"nombre_imagen.png\")))\n",
    "\n",
    "# display(Markdown(\"#### Board Comparisons\"))\n",
    "# display(Image(filename=path.join(IMG_FOLDER, \"nombre_imagen.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 5. Observaciones y resultados\n",
    "\n",
    "### 5.1 Entrenamiento #1 - ULTRA-AGRESIVO\n",
    "\n",
    "<a id=\"train-1\"></a>\n",
    "\n",
    "**Fecha:** 2026-02-16  \n",
    "**Status:** ‚úÖ COMPLETADO\n",
    "\n",
    "#### Par√°metros usados:\n",
    "\n",
    "```python\n",
    "# Configuraci√≥n cr√≠tica\n",
    "REWARD_FUNCTION = \"discount\"      # Cambio de \"propagate\"\n",
    "LOSS_APPROACH = \"only_select\"     # Cambio de \"combined_avg\"\n",
    "ARCHITECTURE = QuartoCNN_uncoupled  # Mantener\n",
    "\n",
    "# Hiperpar√°metros DQN\n",
    "LR = 3e-4                         # Cambio de 5e-4\n",
    "GAMMA = 0.95                       # Cambio de 0.90\n",
    "TAU = 0.01                         # Cambio de 0.005\n",
    "\n",
    "# Generaci√≥n de experiencia\n",
    "N_LAST_STATES = 8                  # Cambio de 6\n",
    "TEMPERATURE_EXPLORE = 0.7          # Cambio de 1.0\n",
    "BATCH_SIZE = 64                    # Mantener\n",
    "MATCHES_PER_EPOCH = 100            # Mantener\n",
    "```\n",
    "\n",
    "#### Resultados finales (√âpoca 3000):\n",
    "\n",
    "| M√©trica | Valor observado | Entrenamiento #2 | Cambio |\n",
    "|---------|----------------|------------------|---------|\n",
    "| **Win Rate vs random** | ~47-50% | ~45% | +5% ‚¨ÜÔ∏è |\n",
    "| **Win Rate vs bot_loss-BT** | ~35-37% | ~37% | Sin cambio ‚û°Ô∏è |\n",
    "| **Q_select estado** | **Colapsado a -1.0** | Colapsado a -1.0 | ‚ùå Sin mejora |\n",
    "| **Q_select diferenciaci√≥n** | **NO** | NO | ‚ùå Sin mejora |\n",
    "| **Q_place diferenciaci√≥n** | **S√ç - CLARA** | D√©bil | ‚úÖ Mejor√≥ |\n",
    "| **Loss final** | ~0.16-0.17 | ~0.25 | ‚¨áÔ∏è -32% mejor |\n",
    "| **Loss estabilidad** | Estable pero oscilante | Estable | M√°s bajo pero no converge |\n",
    "\n",
    "#### An√°lisis detallado por gr√°fica:\n",
    "\n",
    "##### 1. Training Loss\n",
    "- **Observaci√≥n:** Loss oscila entre 0.15-0.18, promedio ~0.16-0.17\n",
    "- **Comparaci√≥n:** 32% m√°s bajo que entrenamiento #2 (~0.25)\n",
    "- **Problema:** No muestra convergencia clara, permanece oscilante durante todas las 3000 √©pocas\n",
    "- **Interpretaci√≥n:** Menor loss indica mejor ajuste, pero la falta de convergencia sugiere que el modelo no est√° alcanzando un √≥ptimo estable\n",
    "\n",
    "##### 2. Win Rate vs Rivals\n",
    "- **vs bot_random (naranja):** \n",
    "  - Oscila entre 45-52%, promedio ~47-50%\n",
    "  - Ligeramente superior al 50% esperado para jugador aleatorio\n",
    "  - **Mejora marginal** vs entrenamiento #2 (~45%)\n",
    "  \n",
    "- **vs bot_loss-BT (azul):**\n",
    "  - Oscila entre 30-40%, promedio ~35-37%\n",
    "  - **Sin mejora** respecto a entrenamiento #2 (~37%)\n",
    "  - Sigue siendo inferior al rendimiento aleatorio esperado\n",
    "\n",
    "- **Conclusi√≥n:** El modelo apenas supera al bot aleatorio y no logra vencer consistentemente al bot_loss-BT\n",
    "\n",
    "##### 3. Q-value Progress ‚ö†Ô∏è **HALLAZGO CR√çTICO**\n",
    "\n",
    "**Q_place (fila superior):**\n",
    "- ‚úÖ **Muestra diferenciaci√≥n clara por recompensa**\n",
    "- R=-1 (izq): Valores concentrados en -0.5 a -1.0 (violeta/morado oscuro)\n",
    "- R=0 (centro): Valores concentrados cerca de 0 (cyan/verde)\n",
    "- R=1 (der): Valores concentrados en 0 a +0.5 (amarillo/verde)\n",
    "- **Interpretaci√≥n:** Q_place **S√ç aprendi√≥** a diferenciar estados seg√∫n resultado del juego\n",
    "- **Paradoja:** Mejor√≥ respecto a entrenamientos anteriores, a pesar de NO ser entrenada (LOSS=\"only_select\")\n",
    "\n",
    "**Q_select (fila inferior):**\n",
    "- ‚ùå **COLAPSO TOTAL - Sin diferenciaci√≥n**\n",
    "- Las tres gr√°ficas (R=-1, R=0, R=1) muestran **exactamente el mismo patr√≥n**\n",
    "- Patr√≥n: Diagonal que comienza en 0 y colapsa a -1.0 alrededor de √©poca 1000\n",
    "- Despu√©s de √©poca 1000: Todos los Q_select = -1.0 (amarillo uniforme en el borde inferior)\n",
    "- **Interpretaci√≥n:** Q_select **NO aprendi√≥ nada √∫til**, colaps√≥ completamente\n",
    "- **Paradoja:** Empeor√≥ o se mantuvo igual, a pesar de SER la √∫nica cabeza entrenada (LOSS=\"only_select\")\n",
    "\n",
    "##### 4. Board Comparisons\n",
    "- ‚úÖ **Bug de visualizaci√≥n corregido:** Ahora se muestran piezas en los tableros\n",
    "- Se observan estados con 8-16 piezas colocadas (estados finales de partida)\n",
    "- Los tableros superiores e inferiores muestran los mismos estados antes/despu√©s de la acci√≥n\n",
    "- **Limitaci√≥n:** Sin m√°s informaci√≥n sobre estrategias espec√≠ficas aprendidas\n",
    "\n",
    "#### Hallazgo parad√≥jico principal:\n",
    "\n",
    "üî¥ **PARADOJA DEL ONLY_SELECT:**\n",
    "\n",
    "Con `LOSS_APPROACH=\"only_select\"` esper√°bamos:\n",
    "- ‚úÖ Q_select aprende (√∫nica cabeza entrenada)\n",
    "- ‚ùå Q_place no aprende (sin gradientes)\n",
    "\n",
    "**Resultado real:**\n",
    "- ‚ùå Q_select colaps√≥ completamente a -1.0\n",
    "- ‚úÖ Q_place mejor√≥ su diferenciaci√≥n\n",
    "\n",
    "**Hip√≥tesis sobre la paradoja:**\n",
    "1. **Q_place mejora pasivamente:** Al usar `only_select`, los gradientes NO actualizan Q_place, pero la red compartida (backbone CNN) s√≠ se actualiza. Esto mejora las features, beneficiando a Q_place indirectamente.\n",
    "\n",
    "2. **Q_select colapsa por problema estructural:** El colapso persistente sugiere un problema m√°s profundo:\n",
    "   - Posible dead ReLU en la cabeza Q_select\n",
    "   - Arquitectura inadecuada para la tarea de selecci√≥n\n",
    "   - Initializaci√≥n problem√°tica que lleva a gradientes que empujan valores a -‚àû\n",
    "\n",
    "3. **REWARD_FUNCTION=\"discount\" insuficiente:** Aunque proporciona mejor se√±al temporal que \"propagate\", no es suficiente para rescatar Q_select."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 6. Conclusiones generales\n",
    "\n",
    "### 6.1 Comparaci√≥n con entrenamientos anteriores\n",
    "\n",
    "| M√©trica | Entrenamiento #1 (Bit√°cora 2) | Entrenamiento #2 (Bit√°cora 3) | Este experimento (Bit√°cora 4) | Evoluci√≥n |\n",
    "|---------|-------------------------------|-------------------------------|-------------------------------|-----------|\n",
    "| Win Rate vs random | ~0.50 | ~0.45 | **~0.47-0.50** | ‚¨ÜÔ∏è Ligera mejora |\n",
    "| Win Rate vs bot_loss-BT | ~0.37 | ~0.37 | **~0.35-0.37** | ‚û°Ô∏è Sin cambio |\n",
    "| Q_select estado | Colapsado a -1.0 | Colapsado a -1.0 | **Colapsado a -1.0** | ‚ùå Sin cambio |\n",
    "| Q_select diferenciaci√≥n | No | No | **No** | ‚ùå Sin cambio |\n",
    "| Q_place diferenciaci√≥n | No | D√©bil | **S√≠ - CLARA** | ‚úÖ Mejor√≥ significativamente |\n",
    "| Loss final | ~0.25 | ~0.25 | **~0.16-0.17** | ‚úÖ -32% mejor |\n",
    "| Loss convergencia | Estancado | Estancado | **Oscilante sin convergencia** | ‚ö†Ô∏è Mejor pero inestable |\n",
    "\n",
    "**Resumen de evoluci√≥n:**\n",
    "- ‚úÖ **Mejoras:** Loss m√°s bajo, Q_place diferencia por recompensa, Win Rate vs random ligeramente mejor\n",
    "- ‚ùå **Sin cambio:** Q_select sigue colapsado, Win Rate vs bot_loss-BT estancado\n",
    "- ‚ö†Ô∏è **Preocupaci√≥n:** Ning√∫n entrenamiento logra superar consistentemente a los baselines\n",
    "\n",
    "### 6.2 Validaci√≥n/Refutaci√≥n de la hip√≥tesis inicial\n",
    "\n",
    "**Hip√≥tesis planteada:**\n",
    "> \"Si cambiamos `REWARD_FUNCTION` y `LOSS_APPROACH` para dar se√±ales de aprendizaje m√°s fuertes y enfocadas, entonces Q_select dejar√° de colapsar y el modelo aprender√° pol√≠ticas efectivas.\"\n",
    "\n",
    "**Resultado:** ‚ùå **HIP√ìTESIS REFUTADA**\n",
    "\n",
    "#### An√°lisis de cada componente:\n",
    "\n",
    "1. **¬øREWARD_FUNCTION=\"discount\" mejor√≥ la se√±al de aprendizaje?**\n",
    "   - ‚úÖ Parcialmente: Loss baj√≥ de ~0.25 a ~0.16\n",
    "   - ‚ùå Insuficiente: Q_select no se rescat√≥\n",
    "\n",
    "2. **¬øLOSS_APPROACH=\"only_select\" rescat√≥ Q_select?**\n",
    "   - ‚ùå **NO:** Q_select colaps√≥ igual o peor\n",
    "   - ‚úÖ **Efecto inesperado:** Q_place mejor√≥ parad√≥jicamente\n",
    "\n",
    "3. **¬øEl modelo aprendi√≥ pol√≠ticas efectivas?**\n",
    "   - ‚ùå **NO:** Win Rate sigue en niveles aleatorios (~50% vs random, ~35% vs bot_loss-BT)\n",
    "   - ‚ö†Ô∏è El modelo NO aprendi√≥ estrategias superiores a juego aleatorio\n",
    "\n",
    "### 6.3 Descubrimiento principal: La Paradoja del Only_Select\n",
    "\n",
    "**Hallazgo m√°s importante del experimento:**\n",
    "\n",
    "Al entrenar SOLO la cabeza Q_select (`LOSS=\"only_select\"`):\n",
    "- La cabeza Q_select (entrenada) **empeor√≥/colaps√≥**\n",
    "- La cabeza Q_place (NO entrenada) **mejor√≥ significativamente**\n",
    "\n",
    "**Implicaciones:**\n",
    "\n",
    "1. **El problema de Q_select es arquitectural, no de entrenamiento:**\n",
    "   - Cambiar hiperpar√°metros (LR, GAMMA, TAU, REWARD) ‚Üí No resuelve el colapso\n",
    "   - Enfocarse exclusivamente en entrenarla ‚Üí No resuelve el colapso\n",
    "   - **Conclusi√≥n:** Hay un problema estructural en la cabeza Q_select\n",
    "\n",
    "2. **Q_place aprende \"pasivamente\" del backbone compartido:**\n",
    "   - Aunque sus pesos finales no reciban gradientes, se beneficia de las mejores features\n",
    "   - El backbone CNN se entrena con gradientes de Q_select\n",
    "   - Estas features mejoradas permiten a Q_place diferenciar mejor los estados\n",
    "\n",
    "3. **LOSS=\"combined_avg\" podr√≠a ser mejor que \"only_select\":**\n",
    "   - Entrenar ambas cabezas simult√°neamente podr√≠a evitar el colapso\n",
    "   - El promedio de losses podr√≠a estabilizar el entrenamiento\n",
    "\n",
    "### 6.4 Causas ra√≠z del fracaso persistente\n",
    "\n",
    "Despu√©s de 3 entrenamientos fallidos (bit√°coras 2, 3 y 4), las causas ra√≠z son:\n",
    "\n",
    "#### 1. Problema arquitectural en Q_select ‚ö†Ô∏è **CR√çTICO**\n",
    "- **Evidencia:** Colapso persistente independiente de:\n",
    "  - Funci√≥n de recompensa (propagate vs discount)\n",
    "  - Loss approach (combined_avg vs only_select)\n",
    "  - Hiperpar√°metros (LR, GAMMA, TAU, BATCH_SIZE)\n",
    "  - Cantidad de datos (N_LAST_STATES=2‚Üí6‚Üí8)\n",
    "\n",
    "- **Posibles causas t√©cnicas:**\n",
    "  - Dead ReLU neurons en la cabeza Q_select\n",
    "  - Inicializaci√≥n inadecuada de pesos\n",
    "  - Desbalance en tama√±o de espacio de acci√≥n (select > place)\n",
    "  - Falta de normalizaci√≥n en las salidas\n",
    "\n",
    "#### 2. Funci√≥n de recompensa a√∫n insuficiente\n",
    "- \"discount\" es mejor que \"propagate\" (loss baj√≥ 32%)\n",
    "- Pero a√∫n no proporciona se√±al suficientemente clara\n",
    "- **Hip√≥tesis:** Quarto requiere recompensas shaped intermedias (ej: bonus por amenazas creadas)\n",
    "\n",
    "#### 3. Arquitectura uncoupled posiblemente sub√≥ptima\n",
    "- Las cabezas desacopladas no comparten aprendizaje eficientemente\n",
    "- Q_select podr√≠a beneficiarse de se√±ales de Q_place\n",
    "- **Restricci√≥n:** No podemos cambiar arquitectura (limitaci√≥n del proyecto)\n",
    "\n",
    "### 6.5 Impacto de los cambios individuales\n",
    "\n",
    "| Cambio | Impacto observado | Conclusi√≥n |\n",
    "|--------|------------------|------------|\n",
    "| REWARD_FUNCTION=\"discount\" | Loss -32%, Q_place mejor√≥ | ‚úÖ √ötil, pero insuficiente |\n",
    "| LOSS_APPROACH=\"only_select\" | Q_select colaps√≥, Q_place mejor√≥ | ‚ùå Contraproducente |\n",
    "| N_LAST_STATES=8 | M√°s diversidad de datos | ‚ö†Ô∏è No se tradujo en mejor aprendizaje |\n",
    "| TEMPERATURE_EXPLORE=0.7 | Mejor calidad de self-play | ‚ö†Ô∏è Mejora marginal en Win Rate |\n",
    "| TAU=0.01, GAMMA=0.95 | Target network m√°s responsivo | ‚ö†Ô∏è No evit√≥ el colapso |\n",
    "| LR=3e-4 | Entrenamiento m√°s estable | ‚úÖ Loss m√°s bajo |\n",
    "\n",
    "**Conclusi√≥n general:** Los cambios mejoraron m√©tricas secundarias (loss, Q_place) pero **NO resolvieron el problema fundamental** (Q_select colapsado, Win Rate bajo).\n",
    "\n",
    "### 6.6 Pr√≥ximos pasos recomendados\n",
    "\n",
    "#### Opci√≥n 1: Investigaci√≥n arquitectural de Q_select üî¨ **RECOMENDADA**\n",
    "\n",
    "**Problema:** Q_select colapsa sistem√°ticamente ‚Üí Requiere an√°lisis profundo\n",
    "\n",
    "**Acciones:**\n",
    "1. **Inspeccionar pesos de la cabeza Q_select:**\n",
    "   - Cargar checkpoint de √©poca 500 (antes del colapso)\n",
    "   - Cargar checkpoint de √©poca 1500 (despu√©s del colapso)\n",
    "   - Comparar distribuci√≥n de pesos: ¬øhay dead neurons?\n",
    "\n",
    "2. **Analizar gradientes durante entrenamiento:**\n",
    "   - Agregar logging de gradientes en Q_select\n",
    "   - Verificar si hay gradient vanishing/exploding\n",
    "   - Revisar si ReLUs se activan correctamente\n",
    "\n",
    "3. **Probar cambios arquitecturales menores:**\n",
    "   - Cambiar ReLU ‚Üí LeakyReLU en Q_select\n",
    "   - Agregar Batch Normalization antes de Q_select\n",
    "   - Cambiar inicializaci√≥n de pesos (He ‚Üí Xavier)\n",
    "\n",
    "#### Opci√≥n 2: Volver a LOSS=\"combined_avg\" con discount üîÑ\n",
    "\n",
    "**Hip√≥tesis:** El promedio de ambas cabezas podr√≠a estabilizar Q_select\n",
    "\n",
    "**Par√°metros sugeridos:**\n",
    "```python\n",
    "REWARD_FUNCTION = \"discount\"      # Mantener (mejor que propagate)\n",
    "LOSS_APPROACH = \"combined_avg\"    # Volver a entrenar ambas\n",
    "GAMMA = 0.95                       # Mantener\n",
    "LR = 3e-4                          # Mantener\n",
    "TAU = 0.01                         # Mantener\n",
    "N_LAST_STATES = 8                  # Mantener\n",
    "TEMPERATURE_EXPLORE = 0.7          # Mantener\n",
    "```\n",
    "\n",
    "**Justificaci√≥n:** Q_place mejor√≥ cuando NO fue entrenada directamente, sugiere que el entrenamiento conjunto podr√≠a ser beneficioso.\n",
    "\n",
    "#### Opci√≥n 3: Shaped rewards intermedias üéØ\n",
    "\n",
    "**Hip√≥tesis:** Recompensas binarias (victoria/derrota) son insuficientes para Quarto\n",
    "\n",
    "**Implementaci√≥n:**\n",
    "- +0.1 por crear amenaza (3 piezas alineadas)\n",
    "- +0.3 por crear doble amenaza\n",
    "- -0.1 por dar pieza que permite victoria inmediata al oponente\n",
    "- +1.0 / -1.0 para victoria/derrota final\n",
    "\n",
    "**Justificaci√≥n:** Juegos con alta ramificaci√≥n (como Quarto) se benefician de recompensas shaped.\n",
    "\n",
    "#### Opci√≥n 4: Cambiar arquitectura (si se levanta restricci√≥n) üèóÔ∏è\n",
    "\n",
    "**Si se permite cambiar arquitectura:**\n",
    "- Probar `QuartoCNN` (cabezas acopladas)\n",
    "- Implementar arquitectura con atenci√≥n\n",
    "- Separar completamente los encoders de place y select\n",
    "\n",
    "### 6.7 Conclusi√≥n final\n",
    "\n",
    "**Estado actual:** Despu√©s de 4 entrenamientos extensivos (bit√°coras 2, 3, 4) con diversos cambios de hiperpar√°metros:\n",
    "- ‚ùå **Q_select permanece colapsada** en todos los escenarios\n",
    "- ‚ùå **Win Rate no mejora** significativamente\n",
    "- ‚úÖ Loss mejor√≥ (pero sin traducirse en mejor gameplay)\n",
    "- ‚úÖ Descubrimos la Paradoja del Only_Select\n",
    "\n",
    "**Diagn√≥stico:** El problema es **arquitectural, no de hiperpar√°metros**. Ajustar LR, GAMMA, TAU, BATCH_SIZE, etc. no resolver√° el colapso de Q_select.\n",
    "\n",
    "**Acci√≥n cr√≠tica necesaria:** **Inspecci√≥n arquitectural profunda de Q_select** antes de continuar con m√°s entrenamientos de hiperpar√°metros.\n",
    "\n",
    "**Lecci√≥n aprendida:** Cambiar muchos hiperpar√°metros simult√°neamente (7 en este caso) sin entender la causa ra√≠z puede generar mejoras superficiales sin resolver el problema fundamental."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ow15jznm57r",
   "metadata": {},
   "source": [
    "## 7. Sistema de Torneo Masivo (3000+ Agentes)\n",
    "\n",
    "### 7.1 Motivaci√≥n\n",
    "\n",
    "Para evaluar y rankear eficientemente los **3001 agentes** entrenados, se desarroll√≥ un sistema de torneo escalable que combina:\n",
    "- **Swiss Tournament System** (emparejamiento por score similar)\n",
    "- **Bradley-Terry Scoring** (ranking estad√≠stico robusto)\n",
    "\n",
    "### 7.2 Problema de escalabilidad\n",
    "\n",
    "#### Round-Robin tradicional (descartado)\n",
    "- **Complejidad:** O(N¬≤) = N √ó (N-1) / 2 matches\n",
    "- **Para 3001 agentes:** 4,501,500 matches\n",
    "- **Tiempo estimado:** ~260 d√≠as (asumiendo 1.5 seg/match)\n",
    "- **Conclusi√≥n:** ‚ùå **IMPRACTICABLE**\n",
    "\n",
    "#### Swiss Tournament (implementado)\n",
    "- **Complejidad:** O(N √ó R) donde R = n√∫mero de rondas\n",
    "- **Para 3001 agentes (1 ronda):** ~1,500 matches\n",
    "- **Tiempo estimado:** ~13-16 minutos\n",
    "- **Conclusi√≥n:** ‚úÖ **VIABLE**\n",
    "\n",
    "### 7.3 Scripts creados\n",
    "\n",
    "#### Script principal: `run_swiss_BT_tournament.py`\n",
    "\n",
    "**Caracter√≠sticas principales:**\n",
    "1. **Swiss Tournament System** con emparejamiento por McMahon scoring (considera √©poca del bot)\n",
    "2. **Bradley-Terry scoring** para ranking estad√≠stico final\n",
    "3. **Sistema BYE** autom√°tico para n√∫mero impar de agentes (3001)\n",
    "4. **Optimizaciones de performance** para ejecuci√≥n r√°pida\n",
    "5. **Guardado solo de CSVs** (no archivos individuales de partidas)\n",
    "6. **Guardado at√≥mico** (solo si el torneo completa exitosamente)\n",
    "\n",
    "**Rutas configuradas:**\n",
    "```python\n",
    "AGENTS_FOLDER = r\"C:\\Users\\bravo\\Documents\\Metodos Numericos Pycharm\\Mech Interp\\TorneoMasivo\\Agentes\"\n",
    "RESULTS_FOLDER = r\"C:\\Users\\bravo\\Documents\\Metodos Numericos Pycharm\\Mech Interp\\TorneoMasivo\\ResutadosTorneo\"\n",
    "```\n",
    "\n",
    "### 7.4 Configuraci√≥n para 15 minutos (3001 agentes)\n",
    "\n",
    "#### Par√°metros del torneo\n",
    "\n",
    "```python\n",
    "# Configuraci√≥n ULTRA-R√ÅPIDA\n",
    "NUM_ROUNDS = 1                    # Solo 1 ronda Swiss\n",
    "MATCHES_PER_PAIRING = 1           # 1 match por emparejamiento\n",
    "ENABLE_COLOR_SWAP = False         # Sin swap = 2x m√°s r√°pido\n",
    "SAMPLE_PERCENTAGE = 100           # Usar todos los emparejamientos\n",
    "TOP_N_BOTS = None                 # Usar todos los 3001 agentes\n",
    "\n",
    "# Par√°metros del juego\n",
    "TEMPERATURE = 0.1                 # Temperatura baja = m√°s determinista\n",
    "DETERMINISTIC = False             # Mantener algo de estocasticidad\n",
    "```\n",
    "\n",
    "#### Optimizaciones Bradley-Terry\n",
    "\n",
    "```python\n",
    "BRADLEY_TERRY_EPOCHS = 5          # Reducido de 100 (10x m√°s r√°pido)\n",
    "BRADLEY_TERRY_THRESHOLD = 1e-2    # Relajado de 1e-6 (converge m√°s r√°pido)\n",
    "BRADLEY_TERRY_NORMALIZE = True    # Normalizar scores finales\n",
    "\n",
    "# Regularizaci√≥n para prevenir divisi√≥n por cero\n",
    "EPSILON = 0.01                    # Regularizaci√≥n vectorizada\n",
    "```\n",
    "\n",
    "### 7.5 Optimizaciones implementadas\n",
    "\n",
    "#### 1. **Reducci√≥n de iteraciones Bradley-Terry**\n",
    "- De 100 √©pocas ‚Üí 5 √©pocas (**20x m√°s r√°pido**)\n",
    "- Threshold relajado: 1e-6 ‚Üí 1e-2 (converge m√°s r√°pido)\n",
    "\n",
    "#### 2. **Vectorizaci√≥n de regularizaci√≥n**\n",
    "```python\n",
    "# ANTES: Double loop O(N¬≤) - MUY LENTO\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        W_reg[i,j] = W[i,j] + EPSILON\n",
    "\n",
    "# DESPU√âS: Operaci√≥n vectorizada - 100x M√ÅS R√ÅPIDO\n",
    "W_regularized = W + EPSILON  # NumPy broadcasting\n",
    "np.fill_diagonal(W_regularized.values, W.values.diagonal())\n",
    "```\n",
    "\n",
    "#### 3. **Sin color swap**\n",
    "- `ENABLE_COLOR_SWAP = False` ‚Üí Solo 1 match por pairing\n",
    "- Reduce matches de ~3,000 ‚Üí ~1,500 (**2x m√°s r√°pido**)\n",
    "- Trade-off: Menos informaci√≥n por pairing\n",
    "\n",
    "#### 4. **Sistema BYE autom√°tico**\n",
    "Para 3001 agentes (n√∫mero impar):\n",
    "- Un bot recibe BYE cada ronda (+1.0 puntos)\n",
    "- Emparejamiento autom√°tico del bot sin pareja\n",
    "- No afecta el tiempo de ejecuci√≥n\n",
    "\n",
    "### 7.6 Tiempo de ejecuci√≥n estimado\n",
    "\n",
    "**Configuraci√≥n ULTRA-R√ÅPIDA (3001 agentes):**\n",
    "\n",
    "| Componente | Tiempo estimado |\n",
    "|------------|----------------|\n",
    "| Carga de agentes | ~30-60 seg |\n",
    "| Ronda 1 (~1,500 matches) | ~10-12 min |\n",
    "| Bradley-Terry (5 epochs) | ~15-30 seg |\n",
    "| Guardado de resultados | ~5-10 seg |\n",
    "| **TOTAL** | **~13-16 minutos** |\n",
    "\n",
    "**F√≥rmula general:**\n",
    "```\n",
    "Tiempo_total ‚âà (N/2) √ó NUM_ROUNDS √ó MATCHES_PER_PAIRING √ó (1 + SWAP) √ó 1.5seg + BT_time\n",
    "```\n",
    "\n",
    "### 7.7 Archivos de salida\n",
    "\n",
    "El script genera los siguientes CSVs en `ResutadosTorneo/`:\n",
    "\n",
    "#### 1. `swiss_BT_tournament_results_YYYYMMDD_HHMMSS.csv`\n",
    "Ranking final con Bradley-Terry scores:\n",
    "\n",
    "| Columna | Descripci√≥n |\n",
    "|---------|-------------|\n",
    "| `rank` | Posici√≥n final (1 = mejor) |\n",
    "| `name` | Nombre del agente |\n",
    "| `bt_score` | Score Bradley-Terry normalizado |\n",
    "| `score` | Puntos Swiss acumulados |\n",
    "| `epoch` | √âpoca de entrenamiento del agente |\n",
    "\n",
    "#### 2. `swiss_BT_tournament_matches_YYYYMMDD_HHMMSS.csv`\n",
    "Log de todos los matches jugados:\n",
    "\n",
    "| Columna | Descripci√≥n |\n",
    "|---------|-------------|\n",
    "| `round` | N√∫mero de ronda |\n",
    "| `player1` | Nombre del bot P1 |\n",
    "| `player2` | Nombre del bot P2 |\n",
    "| `winner` | Nombre del ganador |\n",
    "| `outcome` | Resultado (1=P1 gana, 0=P2 gana) |\n",
    "\n",
    "#### 3. `swiss_BT_tournament_config_YYYYMMDD_HHMMSS.json`\n",
    "Configuraci√≥n exacta del torneo (para reproducibilidad):\n",
    "- Todos los par√°metros usados\n",
    "- Timestamp de ejecuci√≥n\n",
    "- N√∫mero de agentes participantes\n",
    "- Versi√≥n del script\n",
    "\n",
    "### 7.8 Documentaci√≥n adicional\n",
    "\n",
    "Se crearon documentos de referencia:\n",
    "\n",
    "| Archivo | Descripci√≥n |\n",
    "|---------|-------------|\n",
    "| [SWISS_BT_README.md](SWISS_BT_README.md) | Documentaci√≥n completa del sistema de torneo |\n",
    "| [CONFIGURACIONES_RAPIDAS.md](CONFIGURACIONES_RAPIDAS.md) | Gu√≠a de configuraciones predefinidas (15 min, 1 hora, 1 d√≠a, etc.) |\n",
    "\n",
    "### 7.9 C√≥mo ejecutar el torneo\n",
    "\n",
    "```bash\n",
    "# Desde el directorio del proyecto\n",
    "python run_swiss_BT_tournament.py\n",
    "```\n",
    "\n",
    "**Requisitos previos:**\n",
    "1. Tener 3001 agentes (archivos .pth) en `TorneoMasivo/Agentes/`\n",
    "2. Crear carpeta de resultados: `TorneoMasivo/ResutadosTorneo/`\n",
    "3. Asegurar que los agentes tengan nombres con formato: `bot_epoch_XXXX.pth`\n",
    "\n",
    "**Monitoreo durante ejecuci√≥n:**\n",
    "- El script muestra progreso en tiempo real\n",
    "- Logs detallados en consola\n",
    "- Si hay errores, el torneo se aborta (no guarda resultados parciales)\n",
    "\n",
    "### 7.10 Pr√≥ximas mejoras posibles\n",
    "\n",
    "Para experimentos futuros:\n",
    "\n",
    "1. **Mayor precisi√≥n** (si hay tiempo):\n",
    "   - Aumentar `NUM_ROUNDS` a 3-5 rondas\n",
    "   - Habilitar `ENABLE_COLOR_SWAP = True`\n",
    "   - Aumentar `MATCHES_PER_PAIRING` a 2-3\n",
    "\n",
    "2. **An√°lisis de clusters**:\n",
    "   - Agrupar bots por nivel de skill (usando BT scores)\n",
    "   - Identificar √©pocas √≥ptimas de entrenamiento\n",
    "\n",
    "3. **Head-to-head entre top bots**:\n",
    "   - Extraer top 100 bots del torneo\n",
    "   - Hacer round-robin completo solo entre ellos\n",
    "\n",
    "4. **Validaci√≥n estad√≠stica**:\n",
    "   - Bootstrap de rankings para intervalos de confianza\n",
    "   - An√°lisis de sensibilidad a par√°metros BT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
